%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% https://www.biomedcentral.com/getpublished                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% https://miktex.org/                                             %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

%\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
\documentclass{bmcart}

%%% Load packages
\usepackage{amsthm,amsmath}
% \RequirePackage[numbers]{natbib}
% \RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
\usepackage[noadjust]{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{array,etoolbox,multirow}
\usepackage{float}
\usepackage{caption} 
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{lscape}
\usepackage{lineno,hyperref,longtable}
\usepackage[toc,page]{appendix}
\usepackage{rotating}
\usepackage{bookmark}
% \captionsetup[table]{skip=6pt}


% \usepackage[applemac]{inputenc} %applemac support if unicode package fails
% \usepackage[latin1]{inputenc} %UNIX support if unicode package fails

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \def\includegraphic{}
% \def\includegraphics{}

%%% Put your definitions there:
\startlocaldefs
\endlocaldefs

%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Application of Microservices Patterns to Big Data Systems}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
  addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
  corref={aff1},                       % id of corresponding address, if any
% noteref={n1},                        % id's of article notes, if any
  email={jane.e.doe@cambridge.co.uk}   % email address
]{\inits{J.E.}\fnm{Jane E.} \snm{Doe}}
\author[
  addressref={aff1,aff2},
  email={john.RS.Smith@cambridge.co.uk}
]{\inits{J.R.S.}\fnm{John R.S.} \snm{Smith}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgdiv{Department of Science},             % department, if any
  \orgname{University of Cambridge},          % university, etc
  \city{London},                              % city
  \cny{UK}                                    % country
}
\address[id=aff2]{%
  \orgdiv{Institute of Biology},
  \orgname{National University of Sciences},
  %\street{},
  %\postcode{}
  \city{Kiel},
  \cny{Germany}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{artnotes}
%%\note{Sample of title note}     % note to the article
%\note[id=n1]{Equal contributor} % note, connected to author
%\end{artnotes}

\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                           %%
%% The Abstract begins here                  %%
%%                                           %%
%% Please refer to the Instructions for      %%
%% authors on https://www.biomedcentral.com/ %%
%% and include the section headings          %%
%% accordingly for your article type.        %%
%%                                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract} % abstract
  % The panorama of data is ever evolving, and big data has emerged to become one of the most hyped terms in the industry. Today, users are the perpetual producers of data that if gleaned and crunched, will yield game-changing patterns. This has introduced an important shift about the role of data in organizations and many strived to harness to power of this new material. Howbeit, institutionalizing data is not an easy task and requires the absorption of a great deal of complexity. According to various sources, it is estimated that only 13\% of organizations succeeded in delivering on their data strategy. Among the root challenges, big data system development and data architecture are prominent. To this end, this study aims to facilitate data architecture and big data system development by applying well-established patterns of microservices architecture to big data systems. This objective is achieved by two systematic literature reviews, and infusion of results through thematic synthesis. The result of this work is a series of theories that explicates how microservices patterns could be useful for big data systems. These theories are then validated through a semi-structured interview with experts from the industry. The findings emerged from this study indicates that big data architecture can benefit from many principles and patterns of microservices architecture. 
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{big data}
\kwd{microservices}
\kwd{microservices patterns}
\kwd{big data architecture}
\kwd{data architecture}
\kwd{data engineering}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
%\end{fmbox}% uncomment this for two column layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                            %%
%% The Main Body begins here                  %%
%%                                            %%
%% Please refer to the instructions for       %%
%% authors on:                                %%
%% https://www.biomedcentral.com/getpublished %%
%% and include the section headings           %%
%% accordingly for your article type.         %%
%%                                            %%
%% See the Results and Discussion section     %%
%% for details on how to create sub-sections  %%
%%                                            %%
%% use \cite{...} to cite references          %%
%%  \cite{koon} and                           %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}      %%
%%                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%


\section{Introduction}


Since the dawn of internet and world wide web, humanity has witnessed an unprecedented degree of connection. The proliferation of digital devices pervaded with powerful software applications have created cyber communities that constantly mutate \cite{AtaeiACIS,AtaeiBigDataEnvirons}. In a world where we have network infrastructures that can support up to 250Mbps of data transmission, and smart phones and internet of things (IOT) devices that can have processing power of up to 3 Ghz, data becomes ubiquitous, the quantum that lays the foundation of the nexus \cite{AtaeiApsec}. According to InternetLiveStates.com \cite{internet2019internet}, only in one second, there are 9,878 tweets sent, 1,138 instagram photos uploaded, 3,117,720 emails sent, 99,738 Google searches made, and 94,144 Youtube videos viewed. That is, if it has taken 5 second to read the preceding paragraph, during that time, 15,588,600 emails are sent.

Driven by the ambition to harness the power of this large amount of data, the term BD was coined \cite{lycett2013datafication}. BD initially emerged to address the challenges associated with various characteristics of data such as velocity, variety, volume and variability \cite{AtaeiBigDataEnvirons}. BD is the practice of extracting patterns, theories, and predictions from a large set of structured, semi-structured, and unstructured data for the purposes of business competitive advantage \cite{AtaeiHype,Huberty}. BD is a game-changing innovation, heralding the dawn of a new data-oriented industry. Nonetheless, BD is not a magical wand that can enchant any business process. While a lot of opportunities exist in BD, subsuming an emergent and rather high-impacting technology like BD to current state of affairs in organizations, is a daunting task. 

According to recent survey from Databricks, only 13\% of the organizations excel at delivering on their data strategy \cite{DataBricksSurvey}. Another survey by NewVantage Partners indicated that only 24\% organization have successfully gone data-driven \cite{NewVantageSurvey}. This survey also states that only 30\% of organizations have a well established strategy for their BD endeavour. In addition, surveys from McKinsey \& Company \cite{analytics2016age} and Gartner \cite{Nash} further support these numbers, which illuminates on the scarcity of successful BD implementations in the industry.

There are various challenge for adoption of BD such as `data architecture', `rapid technology change', `lack of sufficiently skilled data engineers', and `organization's cultural challenges of becoming data-driven' \cite{AtaeiBigDataEnvirons, Singh}. Among these challenges, 'data architecture' is highlighted. A successful BD system is built upon a solid data architecture, serving as a blue print that affects data lifecycle management, guides data integration, control data assets and handle change. Nevertheless, majority of BD systems are developed on-premise as ad-hoc complicated solutions that do not adhere to the practices of software engineering and software architecture \cite{Gorton,Nadal}. As the data ecosystem grows and new technologies and data processing techniques are introduced, the software architect will have a harder time maintaining a solution that addresses the emergent requirements.

To this end, our study aims to address the challenges of data architecture by proposing a reference architecture (RA). RAs are suitable artifacts for development of complex systems as they capture the essence of knowledge for a class of systems and inject it into practices as high-level models that different stakeholders can reflect on. This can potentially create grounds for an immature architecture that is hard to scale, hard to maintain, and raise high-entry blockades \cite{AtaeiApsec}. Since the approach of ad-hoc design to BD system development is not desirable and may leave many architects in the dark, novel data architectures that are designed specifically for BD are required. To contribute to this goal, we explore the notion of RAs and present a distributed domain-driven software RA for BD systems. This RA is called Cybermycelium. The contribution of this work is threefold: 1) explicating design theories underlying current BD systems, 2) explicating design theories that generate the artefact's constructs, and 3) the artefact. 

% uncomment later

% The contents of this paper is presented in the following order: 1) in Section~\ref*{background-section} (background), we provide a brief discussion on what's known about the topic under investigation, the problems associated with it, and the rationale for our study, 2) in Section~\ref*{RM-section} (research methodology), we discuss the integral elements of our research methodology with corresponding justifications, 3) in Section~\ref*{requirements-section} (requirement specification), we discuss the requirements that has led to creation of the artefact, 4) in Section~\ref*{theory-section} (why Cybermycelium), we expand on the problem stated in the background section and provide with theories that explains the challenges of maintaining current BD systems, 5) in Section~\ref*{artifact-section} (Cybermycelium: A Domain-Driven Distributed Reference Architecture for Big Data Systems), we explicate the theories that underpin the artefact development and the artefact itself, 6) in Section~\ref*{evaluation-section} (evaluation), we provide an evaluation of the artefact, 7) in Section~\ref*{discussion-section} (discussion), we elaborate on other BD RAs, and the distinct qualities and limitations of our artefact, 8) in Section~\ref*{conclusion-section} (conclusion), we conclude the study summing up the findings.


% This focus of this study is on data engineering complexities and in specific BD architecture.

% In the past, organization relied on a few technology giants to provide infrastructure and tools necessary for BD, while today there's a plethora of choice from hundreds of providers covering different aspect of data ecosystem from ingestion, to logging, to stream processing, and to visualization \cite{NewVantageSurvey}. Companies are tending more and more towards cloud native architectures for cost reduction, improved efficiency, which in turn resulted in creation of new roles such as chief analytics officer (CAOs) amd chief data officers (CDOs) to channel the organizational BD capabilities toward business value and competitive advantage \cite{rad2017evaluating}.

% So how can one embark on this rather sophisticated journey? what can be a good logical approach to absorb the ever-increasing complexity of BD systems? how can organizations build different stacks to handle data for various workloads such as machine learning (ML), business analytics, data engineering, and streaming?

% We posit that majority of the challenge discussed starts with data architecture \cite{AtaeiACIS}; \cite{AtaeiApsec}. The data ingestion, processing and consumption of different data workloads vary, and sometimes they don't go well together. A company that enacted a data lake and a data warehouse and tries to account for both ecosystems, can be dealing with immense complexity, which in turns impact data teams, which in turn can hinder innovation, create barriers and result in monumental  loss.

% Development and deployment of an efficacious BD system is only the beginning of a BD journey. As data sources increase, variety of data increases, number of data consumers increase, the data store gets confuscated, and this can introduce threats for scalability and maintainability of the system. This also implies that only a handful of hyper-specialized data engineers would understand the system internals, creating silos, and potential miscommunication.



\section{Background} \label{background-section}

In this section, we provide a brief discussion on what is known about BD architectures, articulate the research gap and problems that need addressing, and present with the objective of this research. 

\subsection{Big Data Architectures: State of the art} \label{State of the art}

The available body of knowledge and the knowledge from practice highlight 3 generations of BD architectures;

\begin{enumerate}
    \item \textbf{Enterprise Data Warehouse:} this is perhaps one of the oldest approaches to business intelligence and data crunching and has existed even before the term BD was coined \cite{leonard2011design}. Usually developed as proprietary software, this data architecture pivot on enterprise data warehouse, extract, transform and load (ETL) jobs, and a data visualization software such as Microsoft Power Business Intelligence (BI). As the data sources and consumers grow, this architecture suffers from hard to main ETL jobs, and visualizations that can be created and understood by a certain group of stakeholders, hindering positive impact of data on business. This also means, new transformations will take longer to be added to the workload, the system is monolithic and hard to scale, and only a few group of hyper-specialized individuals are able to operate the system. Moreover, data warehouses have been designed with different assumptions that cannot effectively handle the characteristics of BD.
    \item \textbf{Data Lake}: to address the challenges occurred in the first generation of data architectures, a new BD ecosystem emerged. This new ecosystem revolved around a data lake, in a way that there isn't as much transformations on the data initially, but rather everything is dumped into the data lake and retrieved when necessary. Although data lake architecture reached a higher level of success in comparison to the first generation of data architectures, they are still far from optimal. As data consumer and data providers grow, data engineers will be immensely challenged to avoid creation of data swamp \cite{brackenbury2018draining}, and because there is usually no concept of data owner, the whole stack is usually operated by a group of hyper-specialized data engineers, creating silos, and barriers for gradual adoption. This also means various teams' concerns will often go into data engineers backlog through an intermediary such as business analyst and they will not be in control of how and when they can consume the data they desire. Furthermore, data engineers are usually oblivious of the semantics and value of the data they are processing; they simply do not know how is that data useful or which domain it belongs to. This will overtime decrease the quality of data processing, results in haphazard data management, and make maintenance and data engineering a complicated task. 
    \item \textbf{Cloud Based Solutions:} Given the cost and complexity of running a data lake on-premise alongside the whole data engineering pipeline, and the substantial talent gap currently faced in the market \cite{AtaeiHype}, the third generation of BD architectures tend to revolve around as-a-service or on-demand cloud-based solutions. This generation of architecture tends to be leaning towards stream-processing with architectures such as Kappa or Lambda \cite{lin2017lambda}, or frameworks that unify batch and stream processing such as Apache Beam \cite{ApachBeam} or Databricks \cite{DataBricks}. This is usually accompanied by cloud storage such as Amazon S3, and streaming technologies such as Amazon Kinesis. Whereas this generation tends to solve various issues regarding the complexity and cost of data handling and digestion, it still suffers from the same fundamental architectural challenges. It does not have clear data domains, a group of siloed hyper-specialized data engineers are running them, and data storage through a monolithic data pipelines soon becomes a choke-point.
\end{enumerate}

To discuss the integral facets that embroil these architectures, one must look at the characteristics of these architectures and the ways in which they achieve their ends. Except for one case \cite{AtaeiApsec}, all the architectures and RAs found as the result of this study, were designed underlying monolithic data pipeline architecture with four major components being data consumer, data processing, data infrastructure and data providers.

The process of turning data into actionable insights in these architectures usually follow a similar lifecycle:

\begin{enumerate}
    \item \textbf{Data Ingestion:} system beings to ingest data from all corners of the enterprise, including both transactional, operational and external data. For instance, in a practice management software for veterinaries, data platform can ingest and persist transactional data such as `user interaction with therapeutics', `number of animals diagnosed', or `number of invoices created' and 'medicines dispensed'.
    \item \textbf{Data Transformation:} data captured from the previous step is then cleansed for duplication, quality, and potentially scrubbed for privacy policies. This data then goes through a multifaceted enrichment process to facilitate data analysis. For instance, a journey of the veterinary nurse can be captured at every stage, enriched with demographics and animal breed for regression analysis and aggregate views.
    \item \textbf{Data Serving:} at this stage, data is ready to be served to diverse array of needs ranging from machine learning to marketing analytics, to business intelligence to product analysis and customer journey optimization. In the `veterinary practice management software' example, the data platform can provide real-time data through event backbone system such as Kafka about customers who have applied and have been dispensed restricted veterinary medicine (RVM) to make sure that these transactions comply with the conditions of the registration of these products.
\end{enumerate}

The lifecycle depicted is indeed a high-level abstract view of prevalent BD systems. Howbeit, it highlights an important matter; these systems are all operating underlying monolithic data pipeline architecture that tends to account for all sorts of data in one architectural construct. This means, data that logically belong to different domains are now all lumped together and crunched in one place, making maintainability and scalability a daunting task \cite{monolithToMesh}.

While architectures in software engineering have gone through series of evolution in the industry, adopting a more decentralized and distributed approaches such as microservices architecture, event driven architectures, reactive systems, and domain-driven design \cite{alshuqayran2016systematic}, the data engineering, and in specific BD ecosystems do not seem to be adopting many of these patterns. Evidence collected from this study have proven that attention to decentralized BD systems, metadata, and privacy is deficient. Therefore, the whole idea of `monolithic data pipeline architecture with no clearly defined domains and ownership' brings significant challenges to design, implementation, maintenance and scaling of BD systems. 

To address these issues, we explore a domain-driven distributed RA for BD systems and propose a RA that addresses some of these challenges. This RA is inspired by the advances in software engineering architectures, and in specific microservices, domain-driven design, and reactive systems. 

\subsection{Why reference architecture?}

To justify why we have chosen RAs as the suitable artefact, first we have to clarify two assumptions;

\begin{enumerate}
    \item having a sound software architecture is essential to the successful development and maintenance of software systems \cite{SoftwareArchitectureKazman}
    \item there exist a sufficient body of knowledge in the field of software architecture to support the development of an effective RA \cite{AtaeiACIS}
\end{enumerate}

One of the focal tenets of software architecture is that every system is developed to satisfy a business objective, and that the architecture of the system is a bridge between abstract business goals to concrete final solutions \cite{SoftwareArchitectureKazman}. While the journey of BD can be quite challenging, the good news is that a software RA can be designed, analyzed and documented incorporating best practices, known techniques, and patterns that will support the achievement of the business goals. In this way, the complexity can be absorbed, and made tractable.

Practitioners of complex systems, software engineers, and system designers have been frequently using RAs to have a collective understanding of system components, functionalities, data-flows and patterns which shape the overall qualities of system and help further adjust it to the business objectives \cite{Cloutier,kohler2019towards}. In software product line (SPL) development, RAs are generic artifacts that are configured and instantiated for a particular domain of systems \cite{Derras}. In software engineering, major IT giants like IBM has referred to RAs as the `best of best practices' to address unique and complex system development challenges \cite{Cloutier}. There is a fair amount of literature on RAs, and whereas different authors definition may vary, they all share the same tenets.

A RA is amalgamation of architectural patterns, standards, and software engineering techniques that bridge the problem domain to a class of solutions. This artefact can be partially or completely instantiated and prototyped in a particular business context together with other supporting artefact to enable its use. RAs are often created from previous RAs \cite{AtaeiACIS}. Based on the premises discussed and taking all into consideration, RAs can facilitate the issues of BD architecture and data engineering because of the following reasons;

\begin{enumerate}
    \item RAs can promote adherence to best practice, standards, specifications and patterns
    \item RAs can endow the data architecture team with openness and increase operability, incorporating architectural patterns that ensue desirable predefined quality attributes
    \item RAs can be the best initial start to the BD journey, capturing design issues when they are still cheap
    \item RAs can bring different stakeholders on the same table and help achieve consensus around major technological constructs
    \item RAs can be effective in identifying and addressing cross-cutting concerns
    \item RAs can serve as the organizational memory around design decisions, enlightening next subsequent decisions
    \item RAs can act as a summary and blueprint in the portfolio of software engineers and software architects, resulting in better dissemination of knowledge
\end{enumerate}

\section{Research Methodology} \label{RM-section}

Our research methodology is made up of two major phases. First we explore the body of knowledge in academia and industry to identify architecturally significant requirements (ASR) for BD systems, and secondly we discuss the chosen methodology for developing the artefact


\subsection{Requirement Specification} \label{requirement-spec-methodology}

Architecture aims to produces systems that are addressing specific requirements, and one cannot succeed in designing a successful architecture if requirements are unknown \cite{SoftwareArchitectureKazman}. Therefore, in this section, we strive to firmly define the requirements necessary for the development of Cybermycelium. We present with three integral pieces of information: 

\begin{enumerate}
    \item Determining the type of the requirement 
    \item Identifying the right approach for categorization of the requirements 
    \item Identifying the right approach for presentation of the requirements 
\end{enumerate}

For maximum clarity, we've mapped the following sub-sections against the above mentioned elements. 

\subsection{Rationale for creating Cybermycelium}

The underpinning of our rationale was the challenges that engulfed the current BD architectures discussed in \ref{State of the art}. In this section, we expand on those issues and explain how our artifact would vary and what challenges it will address. The main goal of Cybermycelium is to introduce a new paradigm that shifts away from monolithic centralized data architectures, into a distributed mesh of data products that communicate through coherent interfaces. This implies change in various aspects. In Cybermycelium, centralized ownership of data by hyper-specialized individuals is broken down to decentralized domains where data is actually produced or used. Cybermycelium shifts away from storing data in monolithic data warehouses and data lakes to distributed data sets that can be connected through standardized interfaces. Cybermycelium aims to shift from centralized top-down data governance, to computer-aided federated construct with policies computationally stored. In Cybermycelium, data is more treated as product rather than an asset to be collected; that is, the provider of the data should be accountable for the quality of data.

\subsubsection{Type of requirements}

Precursor to theorizing about the potential of Cybermycelium, we needed to define what are the requirements. System and software requirements come in different flavours and can range from a sketch on a napkin to formal (mathematical) specifications. Therefore, we first needed to identify what kind of requirements is the most suitable for the purposes of this study. To answer this question, we first explored the body of evidence to understand the current classification of software requirements. 

There's been various attempts to defining and classifying software and system requirements. For instance, Sommerville \cite{sommerville2011software} classified requirements into three levels of abstraction that are namely 1) user requirements, 2) system requirements and 3) design specifications. The author then mapped these requirements against user acceptance testing, integration testing and unit testing.While this could satisfy the requirements of this study, we opted for a more general framework provided by Laplante \cite{laplante2017requirements}. In Laplante's approach, requirements are categorized into three categories of 1) functional requirements, 2) non-functional requirements, and 3) domain requirements. 

Our objective is to define the high-level requirements of BD systems, thus we do not fully explore 'non-functional' requirements. Majority of non-functional requirements are emerged from the particularities of an environment, such as a banking sector or healthcare, and do not correlate to our study. Therefore, our primary focus is on functional and domain requirements and secondly on non-functional requirements.

\subsubsection{Categorizing requirements} 

After having filtered out the right type of requirement, we then sought for a rigorous and relevant method to categorize the requirements. For this purpose, we followed the well-established categorization method based on BD characteristics, that is the 5Vs. These 5Vs are velocity, veracity, volume, variety and value \cite{Bughin2016, rad2017big}. Nadal et al. \cite{nadal2017software} have underpinned their RA on these characteristics and requirements that goes with them. Moreover, NIST BD Public Working Group embarked on a large scale study to extract requirements from variety of application domains such as healthcare, life sciences, commercial, energy, government, and defense. The result of this study was the formation of general requirements under seven categories. In another effort by Volk et al. \cite{volk2020identifying}, nine use cases for BD projects are identified by collecting theories and use cases from the literature and categorizing them using a hierarchical clustering algorithm. Bashari et al. \cite{bashari2016security} focused on the security and privacy requirements of BD systems, Yu et al. presented the modern components of BD systems \cite{yu2019components}, Eridaputra et al. \cite{eridaputra2014modeling} created a generic model for BD requirements using goal oriented approaches, and Al-jaroodi et al. \cite{al2016characteristics} investigated general requirements to support BD software development. 

We've also studied other RAs developed for BD systems to understand general requirements. In one study, Ataei et al. \cite{ataei2020big} assessed the body of evidence and presented with a comprehensive list of BD RAs. This study helped us realize the spectrum of BD RAs, how they are designed and the general set of requirements. By analyzing these studies and by evaluating the design and requirement engineering required for BD RAs, we adjusted our initial categories of requirements and added security and privacy to it.

\subsubsection{Present requirements}
After knowing the type and category of requirements, We looked for a rigorous approach to present these requirements. There are numerous approaches used for software and system requirement representation including informal, semiformal and formal methods. For the purposes of this study, we opted for an informal method because it is a well established method in the industry and academia \cite{kassab2014state}. Our approach follows the guidelines explained in ISO/IEC/IEEE standard 29148 \cite{ISO29148} for representing functional requirements. We have also taken inspiration from Software Engineering Body of Knowledge \cite{abran2004software}. However, our requirement representation is organized in term of BD characteristics. 



\subsection{The artefact development methodology}


There are a few studies that have addressed the systematic development of RAs. Cloutier et al. \cite{Cloutier} present a high-level model for RA development through collection of contemporary information and capturing the essence of architectural advancements. In another effort, PuLSE-DSSA is proposed by Bayer et al. \cite{bayer2004definition} in the context of product line development and domain engineering. PulSE-DSSA emphasizes on capturing the existing architectural knowledge. Stricker et al. \cite{stricker2010creating} propose a pattern-based approach for creating an RA. This study revolves around software engineering patterns motivated by the work of Gamma et al. \cite{gamma1995design}; and proposes a structural approach that includes three layers of patterns with well-defined hierarchical relationships. Nakagawa, Martins, Felizardo, and Maldonado \cite{nakagawa2009towards} propose an approach to RA design outside of product line management context that is concentrated towards aspect-oriented systems.

Galster and Avgeriou \cite{galster2011empirically} propose an empirically grounded RA based on two main facets; Existing RAs in practice and available literature on RAs. Along the same vein, Nakagawa et al. \cite{nakagawa2014consolidating} presented ProSA-RA which is a 4 phase methodology that unlike many other methodologies do provide a more comprehensive instructions on RA evaluation. In addition, this methodology benefits from an ecosystem of complementary constructs that aid in RA design and evaluation such as RAModel \cite{nakagawa2012ramodel} and a framework for evaluation of RAs (FERA) \cite{santos2013checklist}.In a recent study, Derras et al. \cite{derras2018reference} propose a schema of practical RA development in the context of software product line and domain engineering. This study is based on capturing knowledge from architectures in practice with attention to variability, configurability and product line development. The findings provide a four-phase process to develop quality driven RAs. This approach is influenced by ISO/IEC 26550 \cite{wg2015iso}.

By analysis and study of all these approaches for design and development of RAs, a common pattern has been witnessed. Whereas some of them are more recent and some belong to years ago, there are commonalities that has been observed. All these approaches are grounded on three main pillars, 1) Existing RAs 2) RAs in literature 3) Architectures in practice. Taking this into consideration and by analyzing the results of the systematic literature review (SLR) conducted by Ataei et al. \cite{AtaeiACIS} we found `Empirically-grounded RAs' proposed by Galster and Avgeriou \cite{galster2011empirically} a suitable methodology, because firstly it's been adopted by many studies, and secondly it's in-line with the goal of our study.

Nevertheless, we did not fully adopt this methodology and rather customized to the needs of this particular research. This is due to some inherent limitations that has been witnessed with the methodology. For instance we could not find a comprehensive guideline on how to identify data sources and how data could be categorized and synthesized into creation of the RA (in the third step of the methodology). Therefore we employed the Nakagawa's information source investigation guidelines and the overall idea of the RAModel. Another limitation we've faced was with evaluation of the RA. As evaluation, second to a sound research methodology is one of the key elements of any good design science research, we had to look for a stronger and more systematic evaluation approach than what is discussed in `empirically grounded RAs' methodology. For this purpose, and inspired by the works of Angelov et al. \cite{angelov2008towards,angelov2014extending}, we first created a prototype of the RA in practice, and then used `The architecture tradeoff analysis method' (ATAM) \cite{kazman1998architecture} to evaluate the artefact.

This research methodology is constituent of 6 phases which are respectively; 1) Decision on the type of the RA 2) Design strategy 3) Empirical acquisition of data 4) Construction of the RA 5) Enable RA with variability 6) Evaluation of the RA. The phrase `empirically grounded' refers to two major elements; firstly the RA should be grounded in well-established and proven principles; secondly, the RA should be evaluated for applicability and validity. These don't only belong to Galster and Avgeriou's methodology, and other researchers such as Cloutier \cite{Cloutier} and Derras et al. \cite{Derras} have promoted the same ideas.

It is worth mentioning that this methodology is iterative, meaning that the results gained from the evaluation phase (6th phase) determines the subsequent iterations until the design reaches saturation.

\subsection{Step1: Decision on type of the RA}

Precursor to any effective RA development, is the decision on type of it. The type of the RA is significant, as it illuminates on information to be collected and the construction of the RA in later phases. The selection on the type of RA for the purposes of this study is based on two dimensions; the classification framework proposed by Angelov et al. \cite{angelov2009classification} and the usage context \cite{angelov2008contracting}.

Based on the classification framework proposed by Angelov et al. \cite{angelov2009classification}, five types of RA are defined. This framework has been developed with the goal of supporting analysis of RAs with regards to context, goal, and the architecture specification/design relationships. It is based on 3 major dimensions namely context, goals, and design, each having their own corresponding sub-dimensions. These dimensions and sub-dimensions are derived by the means of interrogatives (the usage of interrogates is a well-established practice for problem analysis).

The interrogatives ‘When’, ‘Where’, and ‘Who’ have been used to address the ‘context’, ‘Why’ has been used to address ‘goal’, and ‘How’ and ‘What’ have been used to address ‘design’ dimension. The outcome of the study categorizes RAs in two major groups; 1) standardization RAs and 2) Facilitation RAs. This framework has been chosen because it is completely in-line with the purposes of this study and aims to demarcate a clear domain for the RA to be developed. The comprehensive classification of the RAs with examples in practice illuminates on how different RAs are playing roles in the industry and how they are classified. This brings clarity on what should be developed and what boundaries should be drawn.

By reading the results of the recent SLR conducted by Ataei et al. on BD RAs \cite{AtaeiACIS}, we've added more examples of the RAs on top of what was provided by Angelov \cite{angelov2009classification}, and provided an updated list of RA classifications with examples. This list can be found at \ref{appendixA:RA-classification}.

The domain-driven distributed BD RA chosen for the purposes of this study pursues two major goals; 1) supporting the development of BD systems 2) enabling an effective and scalable data architecture. Therefore, the outcome artefact will be a BD RA that is a classical standardization RA designed to be implemented in multiple organizations.

\subsection{Step2: Selection of Design Strategy}

Angelov et al. \cite{angelov2008towards} and Galster et al.\cite{galster2011empirically} have both presented that RAs can have two major design strategies to them; 1) RAs that are designed from scratch (practice driven), 2) RAs that are based on other RAs (research driven). Designing RAs from scratch is rare, and usually takes place in an emergent domain that have not perceived a lot of attention. On the other hand, most RAs today are the amalgamation of a priori concrete architectures, models, patterns, best practices, and RAs, that together provide a compelling artefact for a class of problems.

RAs developed from scratch tend to create more prescriptive theories, whereas RAs developed based on available body of knowledge tends to provide with more descriptive design theories. The RA designed for the purposes of this study is a research-based RA based on existing RAs, concrete architectures, and best practices.

\subsection{Step 3: Empirical Acquisition of Data }

As aforementioned, due to the limitation witnessed by this research methodology, we have augmented this phase, and increased the systematicity and transparency of data collection and synthesis through various academic methods such as SLR.

This phase is made up of three major undertakings; 1) identification of data sources; 2) capturing data sources; 3) synthesis of data sources.

\subsubsection{Identification of data sources}

To identify suitable data sources, we've employed the first step of ProSA-RA methodology `information source investigation'. This step is an endeavour to capture focal and ancillary knowledge and theories that revolve around the target domain, and lay the ground of RA development.

To unearth the architectural quanta, and to highlight gradations between various approaches to BD system development, we've selected most relevant sources as the followings;

\begin{enumerate}
    \item{\bf{Practice-led conferences:}} given that majority of recent advancements for emerging technologies such as microservices architecture \cite{gan2019open, laigner2021data, laigner2021data} and BD are coming from virtually hosted practice-led conferences, we've chosen some of the best conferences hold world-wide for the purposes of data collection. These conferences are 1) Qcon \cite{QCON} 2) State of Data Mesh by ThoughtWorks \cite{ThoughtWorks} 3) Worldwide Software Architecture Summit'21 \cite{Geekle} and 4) Kafka Summit Europe 2021 \cite{KafkaSummit}. Our objective was to capture the frontiers of software architecture and emerging approaches currently being practiced in IT giants such as Google, Facebook and Netflix. Among all the speech in these conferences, we looked for topics that entailed or were related to the keywords `emergent software architecture trends', `distributed software architecture', `BD software architecture' and `domain-driven design'. We used the software Nvivo to code the transcripts from the conference videos. We used the aforementioned keywords as the codes and associated different texts, summative, essence-capturing sentences, and evocative attributes to them.
    
    \item{\bf{Publications:}} in order to capture evidence from the body of knowledge, we conducted a SLR, following the guidelines of PRISMA presented by Moher et al. \cite{moher2015preferred}, and Kitchenham et al. \cite{kitchenham2015evidence}. Although PRISMA is a comprehensive guidelines for conducting a SLR, it is derived from the healthcare community and is driven by assumptions that may not be thoroughly relevant to software engineering and information system researchers. To this end, we adopted some guidelines from Kitchenham et al. for evidence based software engineering.
    
    The main objective of this SLR was to highlight common architectural constructs found among all the BD RAs. This SLR is build on top of our recent work \cite{AtaeiACIS} that covered all the RAs by 2020.

    The initial SLR included IEEE Explore, ScienceDirect, SpringerLink, ACM library, MIS Quarterly, Elsevier, AISel as well as citation databases such as Scopus, Web of Science, Google Scholar, and Research Gate. The SLR search keywords used were `Big Data Reference Architectures’, `Reference Architectures in the domain of Big Data’, and `Reference Architectures and Big Data’. We followed the same methodology, but this time for the years 2021 and 2022. Our aim was to find out if there has been any new BD RA published.

    By the result of this SLR, we've found 3 more BD RAs \cite{AtaeiApsec,castellanos2021smart,sang2017simplifying} and we've added two new standards \cite{ISO20546,ISO20547} to further solidify our study. Converging these new SLR with the old, covering the years 2010-2022, we've pooled 89 literature in the primary phase, and another 10 by snowballing and citation searching. These 99 literature then went through our inclusion, exclusion criteria. These criteria are as blow;

    % \begin{itemize} 
        \textit{Inclusion criteria:}
        \begin{enumerate}
            \item Primary and secondary studies between Jan 1st 2020 and Sep 1st 2022 focused on the topics of BD RA, BD architecture, and BD architectural components
            \item Research that indicates the current state of RAs in the field of BD and demonstrates possible outcomes
            \item Studies that are scholarly publications, books, book chapters, thesis, dissertations, or conference proceedings 
            \item Grey literature such as white paper that includes extensive information on BD RAs
        \end{enumerate}
        \textit{exclusion criteria:}
        \begin{enumerate}
            \item Informal literature surveys without any clearly defined research questions or research process
            \item Duplicate reports of the same study (a conference and journal version of the same paper)
            \item Short papers (less than 5 pages)
            \item Studies that are not written in English
        \end{enumerate}

    % \end{itemize}

\end{enumerate}

The screening process was conducted by each researcher separately. Disagreement among researchers were resolved using Krippendorff’s alpha \cite{krippendorff2011computing}. Our aim was not to get involved in a very complicated statistics model, so we've done most of the computations using SPSS, specifically with Hayes’ Macro. Our $ \alpha $ value was within the acceptable range (above 80). 

After excluding papers based on inclusion and exclusion criteria, and as suggested by Kitchenham et al. \cite{kitchenham2015evidence}, we assessed studies based on their quality. For this purposes, and inspired by Critical Appraisal Skills Programme CASP \cite{CASP}, we developed our quality framework based on 7 criteria. These 7 criteria tested literature on 4 major areas that can critically affect the quality of the studies. These categories and the corresponding criteria are as following;

\begin{enumerate}
    \item \emph{Minimum quality threshold:} 
    \begin{enumerate}
        \item Does the study report empirical research or is it merely a `lesson learnt' report based on expert opinion ?
        \item The objectives and aims of the study is clearly communicated, including the reasoning for why the study was undertaken ? 
        \item Does the study provide with adequate information regarding the context in which the research was carried out ?
    \end{enumerate}
    \item \emph{Rigour:}
    \begin{enumerate}
        \item Is the research design appropriate to address the objectives of the research ?
        \item Is there any data collection method used and is it appropriate ?
    \end{enumerate}
    \item \emph{Credibility:}
      \begin{enumerate}
        \item Does the study report findings in a clear and unbiased manner ? 
     \end{enumerate}
    \item \emph{Relevance:}
    \begin{enumerate}
        \item Does the study provides value for practice or research ?
     \end{enumerate}
\end{enumerate}

Taken all together, these 7 criteria gave us a measure of the extent to which a particular study's findings could make a valuable contribution to the review. These criteria were disseminated as a checklist among researchers with value for each property being dichotomous, that is `yes’ or `no’ in two
phases. In the first phase, researchers only assess the quality
based on the first major area (minimum quality threshold).
If the study passed the first phase, it would then go into the
second phase, where it was assessed for credibility, rigour
and relevance. The quality is agreed if 75\% of the responses
are positive for any given study with at least 75\% inter-rater
reliability.
% \item Quality assessment:
% \begin{enumerate}
%     \item is the study rich in terms of relevance to practice?
%     \item does the study create related design/design science or kernel theories?
%     \item does the study entail sufficient data?
%     \item does the study discuss the recent trends in BD domain?
%     \item is the study based on primary data and is internationally focused?
% \end{enumerate}


\subsubsection{Data Synthesis}

After pooling the studies, 13 studies have been removed based on inclusion  and exclusion criteria. From there on, 76 studies have been assessed for eligibility based on the quality framework and the inclusion criteria. The result of this process handed over 63 studies from this branch (identification of studies visa database and registers). From the other branch (identification of data via other methods), 10 records identified through citation searching. These reports have been assessed through the same quality framework, inclusion and exclusion criteria, yielding 5 studies. Together 68 studies pooled for this SLR as depicted in Figure \ref{fig:PRISMA}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=10cm]{PRISMA.JPG}
    \caption{PRISMA flowchart}
    \label{fig:PRISMA}
\end{figure}


These 68 studies are comprising of journal papers, conference papers, book chapters, tech reports, tech surveys white papers, standards, master dissertation, and PhD thesis. Out of the pool of these studies, 39.4\% are from IEEE Explore, 4.4\% are from ScienceDirect, 23.5\% are from Springerlink, 13.2\% are from ACM, and 29.4\% are from other sources such as citation search, Google Scholar and Research Gate. 30 journal articles, 14 conference papers, 6 whitepapers, 2 ISO standards, 14 book chapters, and 2 postgraduate studies have been selected. 26\% of these studies are from the year 2010-2013, 33\% are from the years 2013-2015, and 51\% are from the years 2016-2022. These stats are portrayed in Figure \ref{fig:SLRStats}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=12cm]{databases-statitistic.jpg}
    \caption{SLR statistics}
    \label{fig:SLRStats}
\end{figure}


By this stage, the research objective is set, studies are pooled, assessed and refined, thus the research embarked on the actual synthesis of data. For this purpose, we employed thematic synthesis presented by Cruzes et al. \cite{cruzes2011recommended}. An integral element of this phase is data extraction, in which the essence of the studies are obtained in an explicit and consistent manner. We opted for an integrated approach to coding \cite{lofland1971analyzing} using the software Nvivo \cite{nvivo}. All the keywords aforementioned has been created as nodes in the software, which are then associated to relevant sentences in studies. After coding all the studies, the findings have been synthesized to create theories, which in turn emerged themes and patterns. The findings gained from this SLR grounded the foundation for various aspect of the SLR development. To increase transparency, RAs and standards found as the result of this SRL is presented in \ref{appendixA:SLR}. 
   
% \subsection{Construction of the RA}

% Based on the themes, theories, and patterns realized in the previous steps, the process of RA construction took place. Integral to this step was the identification of elements that the RA should contain, how these elements should be synthesized, and how the RA can be portrayed and communicated. To describe our RA, we followed ISO/IEC/IEEE 42010 standard \cite{ISO42010}. This standard pivots on concrete architectures, so we did not 100\% conform to it, but rather the good and relevant parts of it has been taken. For instance, architecture viewpoints, statement of corresponding rules, and expression of the architecture through architecture description languages (ADLs) have had direct impact on the construction of Cybermycelium.

% A key challenge in the development of Cybermycelium was to strike a balance between the specificity of the micro patterns and approaches to system development and general architectural concepts that reflect a view of the system as an array of interrelated entities. Angelove et al. \cite{angelov2012framework} approached this problem by the means of interrogatives through a defined framework that aims to guide the creation of RAs.
% Cloutier et al. \cite{Cloutier} suggest that a RA should entail technical, business and customer context views, whereas Vogel et al. \cite{vogel2009software} provided classified RA views based on the usage context, as industry specific, platform specific, industry crosscutting and product line RAs.

% Stricker et al. \cite{Stricker} expressed their pattern-based RA by adhering several distinct views into one. Chang et al. \cite{Chang} presented NIST BD RA as a system constituent of logical components connected though interoperability interfaces in several fabrics. On the other hand, ISO/IEC/IEEE 42010 refrains from using phrases such as `technical architecture', `physical architecture', or `business architecture'.

% Taking the best evidence from the available body of knowledge, We decided to adhere several views into one and express the RA through a multi-layer modeling language called Archimate. Archimate is a mature modeling language developed by Open Group that provides with a uniform representation of high-level architectural diagram aimed at portraying and delineating enterprise architecture \cite{lankhorst2013language}. Archimate being listed as a standard architecture description language in ISO/IEC/IEEE 42010, is designed based on a set of related concepts that are specialized towards the system at different architectural layers. This means that the architect is enhanced with an integrated architectural tool that visualizes and describes different architecture domains and their underlying relations \cite{lankhorst2010anatomy,engelsman2011extending}.

% Archimate utilizes service-orientation to distinguish and relate the application, business and technology layer and use realization relationships to create relationship between concrete elements and more abstract elements across three layers. In addition, Archimate can be customized to account for varying needs of the architect.

% \subsection{Enabling RA with variability}

% Enabling RA with variability is an important process that helps with the instantiation of it.
% This allows RA to remain useful as a priori artefact when it comes to organization-specific regulations, and regional policies that constrain the architect design decisions \cite{rurua2019representing}.

% Variability management has been studied in the domain of Business Process Management (BPM) \cite{la2009questionnaire,rosemann2007configurable,hallerbach2010capturing}, and Software Product Line Engineering (SPLE) \cite{pohl2005software,chen2011systematic,schmid2004customizable,svahnberg2005taxonomy,sinnema2006covamof}. In BPM, variability management revolves around efficient handling of different variants in business processes, whereas in SPLE, variability management is about modifying and extending the software artefact to account of the requirements of a specific context.

% Clear identification of variability and explicit communication of it improves communication between stakeholders, allows for traceability between variation causes and effects and facilitates the decision making \cite{czarnecki2012cool}. Variation points are decided based on the data collected in previous steps. Galster et al. \cite{galster2011empirically} sugguest that there are three approaches to enabling variability;

% \begin{enumerate}
%     \item Annotation of the RA
%     \item Variability views
%     \item Variability models
% \end{enumerate}

% We could not find an in-detail explanation of how one should choose the appropriate variability enabling approach. Therefore, inspired by the works of Rurua et al. \cite{rurua2019representing}, we decided to extend the RA with variability, by the means of Archimate annotations. We have achieved this in two steps; first we developed a custom layer that represents focal variability concepts, and then we extended the RA through annotation. The aim of this process is not find all variability points that may emerge in the usage context, but to provide with high-level system related architectural variabilities that an architect may consider for improvement of design and adoption of the RA.

% The variability model is depicted in Fig \ref{variability} by the means of Archimate's motivation layer. This modeling is inspired by the works of Pohl et al. \cite{pohl2005software} and in specific their graphical notation of variability information, and Rurua et al. \cite{rurua2019representing} and in specific, their variability management concepts model. Our variability model can be employed by the architect to be used on variable components of Cybermycelium discussed in \ref{the-artifact}.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=13cm]{variability-model.JPG}
%     \caption{Variability management concepts model}
%     \label{variability}
% \end{figure}

% \subsection{Evaluation of the RA}

% Evaluation of the RA is to ensure that it has achieved the goals stated prior to development, to test is effectiveness and usability, and to make sure that it addresses the identified problems. Two fundamental pillars of the evaluation is the correctness and the utility of the RA and how efficient it can be adapted and instantiated \cite{galster2011empirically}. The quality of RA can be assessed by how it can be transformed into an effective organization-specific concrete architecture. The fact that this RA is built upon former RAs helps making the evaluation steps easier as this research can get inspiration from other studies and their approach to evaluation \cite{sharpe2019industrial}.

% Nevertheless, evaluation of the RAs is a well-known challenge among researchers \cite{angelov2008contracting,Avgeriou,Cioroaica,Maier}. RAs and concrete architectures have distinct qualities. They vary in at least 3 major ways;

% \begin{enumerate}
%     \item RAs are of higher level of abstraction
%     \item In RAs stakeholders are not clearly grouped
%     \item RAs tend to be focused more on architectural qualities
% \end{enumerate}

% While there are many well-established methods for assessing concrete architectures such as Scenario-based Architecture Analysis Method \cite{kazman1994saam}, Architecture Level Modifiability Analysis \cite{Bengtsson}, Performance Assessment of Software Architecture \cite{Williams}, Architecture Trade-off Analysis Method \cite{KazmanATAM}, none of these methods can be directly applied to evaluate RAs. To support this statement, three major issues have been identified. These issues are as follows;

% \begin{enumerate}
%     \item One of the main problems for applying existing evaluation methods to RA is the lack of clearly defined group of stakeholders \cite{angelov2008towards}, while ATAM and other methods are highly dependent on participation of stakeholders for evaluation. Due to the level of abstractness of the RA, reaching various group of stakeholders and persuade them to anticipate in the study, is problematic and does not fit to the timeline of this study. Even more notably, it is unlikely that all stakeholders will unite around a common RA as different members may or may not agree with the overall idea of the RAs, may come from different backgrounds, and may lack architectural visions. In addition, this process may introduce bias.
%     \item Evaluation frameworks and methods for concrete architectures make use of scenarios. Howbeit due to RAs level of abstraction, creation of usable scenarios is difficult. Either a large set of scenarios should be developed covering all the aspects of the RA with regards to specific domain, or a more general scenarios should be developed to cover all the aspects. In the first approach, a large number of scenarios, makes data analysis troublesome and a tedious process. Moreover, the order of prioritization of these scenarios and defining them, and validating them is a problematic task. In the second approach, due to the generality of the scenario, evaluation of effectiveness and usability of the RA becomes difficult and may become incomplete \cite{Avgeriou}. These challenges have been observed even in the evaluation of highly complex concrete architectures in the domain of information systems \cite{bengtsson1998scenario}.
% \end{enumerate}

% Based on the problems discussed above, available methods of architecture analysis are not sufficient in evaluating the RA. This has been addressed by various researchers in the industry.

% In one study, Angelov et al. \cite{angelov2008towards}, modified ATAM and extended it to resonate well with RAs. This process took place by invitation of representatives from leading industries for the evaluation process, and the selection of various contexts and defined scenarios for these contexts. Furthermore, ATAM has been extended to evaluate completeness, buildability and applicability. Howbeit the selection of the right candidate and involving them in the process is a time-consuming and daunting task and may yield incomplete information. In addition, candidates maybe lacking architectural visions, increasing the threat to validity.

% In addition to extending ATAM for RAs, Graaf et al. \cite{graaf2005evaluating} presented an evaluation approach in which SAAM is extended to help reduce the organizational impact of it. In Another study by Maier et al. \cite{Maier}, the evaluation of the RA has been conducted by mapping it against existing concrete architectures described in industrial whitepapers and reports. Along the lines, Galstar et al. \cite{galster2011empirically} suggested reference implementations, prototyping and incremental approach for the validation of the RA.

% Rohling et al. \cite{rohling2019reference} have evaluated their RA by mapping it against the requirements set for the study. This was facilitated by the RA research methodology created by Nakagawa et al. \cite{Nakagawa} and the complementary RAModel \cite{nakagawa2012ramodel}.

% Inspired by all the studies listed, for the purposes of this study, we will first create a prototype of the RA in an actual organizational setup and then we will use ATAM to evaluate the prototype.

% \section{Cybermycelium Software and System Requirements} \label{requirements-section}

% By the result of the processes conducted in Section~\ref{requirement-spec-methodology}, and by carefully evaluating similar approaches to requirement specification, we tailored a set of requirement for the development for Cybermycelium. These requirements are presented in terms of BD characteristics in the following sub-sections.

% \setlist{nosep,leftmargin=1.8cm,before=\vspace{\baselineskip}}

% \subsection{Volume}

% Volume refers to addressing multitude of data for the purposes of storage and analysis. An architecture needs to be elastic enough to address volume demands at different rates. Storing and computing large volume of data with attention to efficiency is a complex process that requires distributed and parallel processing. Therefore, volume requirements for Cybermycelium are as following:

% % in Table \ref{table-requirements}. These requirements are mapped against our components discussed in \ref{the-artifact}.

% \begin{enumerate}[label=\textbf{Vol-\arabic*}]
%     \item System shall support asynchronous, streaming, and batch processing to collect data from centralized, distributed, and other sources
%     \item System shall provide a scalable storage for massive data sets
% \end{enumerate}


% \subsection{Velocity}

% Velocity refers to addressing the rate at which data flows into system for different analytical requirements. Processing of data to expedite the decision-making process quickly on one hand and handling the variety of data and storing them for batch processing, stream processing or micro-batch processing on the other hand bring considerable technical challenge. Therefore, velocity requirements of Cybermycelium are as following: 

% \begin{enumerate}[label=\textbf{Vel-\arabic*}]
%     \item System shall support slow, bursty, and high-throughput data transmission between data sources
%     \item System shall stream data to data consumers in a timely manner
%     \item System shall be able to ingest multiple, continuous, time varying data streams
%     \item System shall support fast search from streaming and processed data with high accuracy and relevancy
%     \item System shall be able to process data in real-time or near real-time manner
% \end{enumerate}


% \subsection{Variety}

% Variety refers to addressing data in different formats, such as structured, unstructured, and semi-structured. Different formats may require different processing techniques, may have different storage requirements and may be optimized in different ways. Hence, an effective BD architecture can handle various data types and enable the processing and transformation of them in an efficient manner. Therefore, the variety requirements of Cybermycelium are as following: 

% \begin{enumerate}[label=\textbf{Var-\arabic*}]
%     \item System shall support data in various formats ranging from structured to semi-structured and unstructured data
%     \item System shall support aggregation, standardization, and normalization of data from disparate sources,
%     \item System shall support adaptations mechanisms for schema evolution
%     \item System may provide mechanisms to automatically include new data sources
% \end{enumerate}


% \subsection{Value}

% Value refers to addressing the process of knowledge extraction from large datasets. Value is perhaps one of the most challenging aspects of BD architecture as it involves a variety of cross-cutting concerns such as data quality, metadata and data interoperability. Gleaning, crunching and extracting value from data, requires an integrated approach of storage and computing. Value requirements for Cybermycelium are as following:

% \begin{enumerate}[label=\textbf{Val-\arabic*}]
%     \item System shall able to handle compute-intensive analytical processing and machine learning techniques
%     \item System shall support two types of analytical processing: batch and streaming
%     \item System shall support different output file formats for different purposes
%     \item System shall support streaming results to the consumers 
% \end{enumerate}

% \subsection{Security and Privacy}

% Security and privacy should be some of the top concerns for the design of any effective BD system. An effective architecture should be secure, adopting the best security practices (principles of least privilege) and in the meantime respect regional and global privacy rules (General Data Protection Regulation). The security and privacy requirements of Cybermycelium are as following:

% \begin{enumerate}[label=\textbf{SaP-\arabic*}]
%     \item System shall protect and retain privacy and security of sensitive data
%     \item System shall have access control, and multi-level, policy-driven authentication on protected data and processing nodes.
% \end{enumerate}

% \subsection{Veracity}

% Veracity refers to keeping a certain level of quality for data. Data veracity refers to truthfulness and accuracy of data; in simpler terms, it is to ensure that data possess qualities necessary for crunching and analysis. Veracity requirements for Cybermycelium are as following: 

% \begin{enumerate}[label=\textbf{Ver-\arabic*}]
%     \item System shall support data quality curation including classification, pre-processing, format, reduction, and transformation
%     \item System shall support data provenance including data life cycle management and long-term preservation
% \end{enumerate}

% % \begin{table}
% %     \centering
% %     \caption{BD system requirements}
% %     \renewcommand*{\arraystretch}{1.4}
% %     \begin{tabular}{|m{1.2cm}|m{10.5cm}|}

% %         \hline

% %         Volume &

% %         \textbf{Vol-1)} System needs to support asynchronous, streaming, and batch processing to collect data from centralized, distributed, and other sources, \textbf{Vol-2)} System needs to provide a scalable storage for massive data sets 
% %         \\
% %         \hline
% %         Velocity & 
        
% %         \textbf{Vel-1)} System needs to support slow, bursty, and high-throughput data transmission between data sources, \textbf{Vel-2)} System needs to stream data to data consumers in a timely manner, \textbf{Vel-3)} System needs to be able to ingest multiple, continuous, time varying data streams, \textbf{Vel-4)} System shall support fast search from streaming and processed data with high accuracy and relevancy, \textbf{Vel-5)} System should be able to process data in real-time or near real-time manner 
% %         \\ 

% %         \hline

% %         Variety & 

% %         \textbf{Var-1)} System needs to support data in various formats ranging from structured to semi-structured and unstructured data, \textbf{Var-2)} System needs to support aggregation, standardization, and normalization of data from disparate sources, \textbf{Var-3)} System shall support adaptations mechanisms for schema evolution, \textbf{Var-4)} System can provide mechanisms to automatically include new data sources 
% %         \\

% %         \hline

% %         Value & 
        
% %         \textbf{Val-1)} System needs to able to handle compute-intensive analytical processing and machine learning techniques, \textbf{Val-2)} System needs to support two types of analytical processing: batch and streaming, \textbf{Val-3)} System needs to support different output file formats for different purposes, \textbf{Val-4)} System needs to support streaming results to the consumers 
% %         \\

% %         \hline

% %         Security \& Privacy & 
        
% %         \textbf{SaP-1)} System needs to protect and retain privacy and security of sensitive data, \textbf{SaP-2)} System needs to have access control, and multi-level, policy-driven authentication on protected data and processing nodes. 
% %         \\

% %         \hline
        
% %         % Veracity &
        
% %         % \textbf{Ver-1)} System needs to support data quality curation including classification, pre-processing, format, reduction, and  transformation, \textbf{Ver-2)} System needs to support data provenance including data life cycle management and long-term preservation.
% %         % \\
% %         % \hline
  
% %     \end{tabular}
% %     \label{table-requirements}
% % \end{table}

% % \begin{table}
% %     \centering
% %     \renewcommand*{\arraystretch}{1.4}
% %     \begin{tabular}{|m{1.2cm}|m{10.5cm}|}
        
% %         \hline

% %         Veracity &
                
% %         \textbf{Ver-1)} System needs to support data quality curation including classification, pre-processing, format, reduction, and  transformation, \textbf{Ver-2)} System needs to support data provenance including data life cycle management and long-term preservation.
% %         \\
% %         \hline

% % \end{tabular}
% % \end{table}

% \section{Why Cybermycelium?} \label{theory-section}

% In this section, we present with theories that aim to explain the limtiations of current BD architectures. While we have briefly discussed the current status of BD architectures and failure modes in Section~\ref{State of the art}, this section digs deeper into these challenges. 

% \subsection{A need for a paradigm shift} \label{need for paradigm shift}

% If our aspiration to enhance every business aspect with data needs to come to fruition, we need a different approach to data architecture. Traditional data warehouse approaches to business intelligence, while have addressed the volume and computing aspect of data, have failed to address other characteristics of it; heterogeneity and proliferation of data sources (variety), the speed at which data arrives and needs to be processed (velocity), the rate at which data mutates (variability), and the truth or quality of the data (veracity).


% Suppose company A would want to adopt a BD initiative to embark on this complex endeavour, what would be the first step ? does the company have to worry about high-throughput stream processing ? does it have to worry about regional privacy regulations? does it have to worry about cost-efficient batch processing? perhaps yes to all of these questions, but what's even more integral to the success of the whole endeavour is the underlying data architecture that governs the entire system, its components, their relations to each other, data flow and principles and standards that govern the quality attributes and evolution of the system.

% This architecture and design process if done underlying current prevalent approaches, can result in considerable losses, and may leave managers disappointed. Nevertheless, we don't claim that all these architectures will fail, perhaps some have proven to be successful in a specific context. There are two threats to maintainability and scalability of these systems;

% \begin{enumerate}
%     \item \textbf{Data source proliferation:} as the BD system grows and more data sources are added, the ability to ingest, process, and harmonize all these data in one place diminishes. This in turn, reduces maintainability, makes company reliant on lead data engineers who built the infrastructure, and makes scaling these systems very difficult. The proliferation of data sources if not managed carefully, can result in data swamps as well, which makes understanding data domains, and providing data as a service a complicated task.
%     \item \textbf{Data consumer proliferation:} organizations that utilize rapid experimentation approaches such Hypothesis-Driven Development and Continuos Delivery \cite{hypothesisDev} constantly introduce new use cases for data to be consumed in different domains. This means that variability of the data rises, and the sum of aggregations, projections, and slices increases, which in turn adds more work to the backlog of the data engineering team, slowing down the process of serving the data to consumers. Inability to account for the data consumer demands can be a point of friction in organizations \cite{monolithToMesh}. 
% \end{enumerate}

% To address these challenges, the lead architect or the architecture governance group will then have to choose the right architectural quanta to segregate the monolith. According to Ford et al. \cite{ford2017building}, an architectural quantum is a component of a system with high functional cohesion that is independently deployable; this is also referred to as a service \cite{newman2021building}. The main motivation for segregating the monolith into its architectural quantum, is to parallelize work in various business domain, to reach a better velocity, reduce cost, promote ownership, increase performance and reach higher operational scalability.

% Currently, these architectures are usually segregated into pipelines that each process data differently. While each pipeline has its own responsibility to handle various aspect of the BD system, there is still a high coupling between the pipelines, as `data cleansing' phase cannot start after `data ingestion'. This coupling is even more highlighted when the company is at the stage of rapid experimentation with data sources and would like to explore new domains of insight generation, and this in turn means that delivering new features and values is orthogonal to the axis of change.

% Using the same practice management software example, given that a new class of animals (equine animals as opposed to small animals) should be incorporated for data analysis; the data engineering team should then modify and extend the whole pipeline of ingest, process and serve to account for the particularity of the data captured regarding this new class of animals. New ingestion services required, the schema might change, the veracity checking mechanisms might differ, cleansing varies and more. This implies an end-to-end dependency which affects external teams, slows down processes and make maintenance gradually harder. This implies that using pipeline as an architectural quantum in such a coupled way is perhaps not the most efficient architecture to BD systems.

% Another major issue with the current architectural approaches is that data engineering is usually confined into a team of hyper-specialized individuals who are siloed from the operational units of the organization. These teams, being fully responsible for creating the infrastructure for data processing, are often absent of business knowledge and the domain, which limits their productivity. These individual usually have a limited understanding of the data sources, data provenance, data consumers, the changing nature of the business domains, the overall product vision, and the application side of things; yet they are responsible to provide data for a large array of analytical and operational needs in a timely manner. For instance, given a context in which a product owner and application developer can cooperate, the synergy between the data engineer, application developer, and product owner can bring about more mature decisions that can align the requirements across various technical and logical domains and allow for various stakeholders to contend and communicate their concerns.

% Whereas there could be other factors to be discussed deeply in this paper regarding current BD architectures, our aim is not to explore any further and emphasize more on the solution artefact we've designed to address some of these challenges. This artefact is discussed in the next section.

% \section{Cybermycelium: A Domain-Driven Distributed Reference Architecture for Big Data Systems} \label{artifact-section}

% This section is constituent of two integral elements: theory and artefact. First we begin be exploring the major architectural constructs that underpins our artefact and then we present the artefact and describe its components.

% \subsection{The Theory}

% There are various design and kernel theories employed to justify our artefact and the decisions made. These theories are described in following sub-sections.

% \subsubsection{A paradigm shift: distributed domain-driven architecture}

% Based on the premises discussed in Section~\ref{need for paradigm shift}, one can infer that the idea of monolithic and centralized data pipelines that are highly couped and operated by silos of hyper-specialized BD engineers has limitations and can bring organizations into a bottleneck.

% We therefore, explore a domain-driven distributed and decentralized architecture for BD systems and posit that this architecture can address some of the challenges discussed. This idea is inspired by the advancements in software engineering architecture, and in specific event-driven microservices architecture \cite{EventDrivenMicroServices}, domain-driven design \cite{evans2004domain}, and reactive systems \cite{aceto2007reactive}.

% Data usually comes into two different flavours; 1) operational data which serves the need of an application, facilitates logic, and can include transactional data and 2) analytical data which usually has the temporality to it, and is aggregated to provide with insights.

% These two different flavours, despite being related, have different characteristics and trying to lump them together may result in a morass. To this end, Cybermycelium realizes the varying nature between these two planes and respects the difference. Cybermycelium aims to transfigure current architectural approaches by proposing an inversion of control, and a topology based on product domains and not the technology \cite{dataMeshArticle}. Our proposition is that handling two different archetypes of data, should not necessarily result in siloed teams, heavy backlogs, and a coupled implementation.

% To further elucidate on this matter, we take the example of the microservices architecture. As the industry sailed away from the monolithic n-tier architectures into Service Oriented Architectures (SOA), organizations faced a lot of challenges. One prevalent issue was around the maintenances of Enterprise Service Bus (ESB) or SOA bus, which is the locus of aggregation. While the aggregation layer could be written very thin, the reality is that the transformation of XML and logical operations started to bloat the SOA bus. This added a new level of coupling between internal and external elements of the system as a whole \cite{di2017architecting,zimmermann2017microservices,waseem2020systematic}.

% Microservices architecture, being the evolution of SOA, move away from smart pipelines into dumb pipelines and smart services removing the need for the locus of aggregation and control. Moreover, there was no business logic written in the pipelines, and each service was segregated usually with the help of domain-driven design.

% Whereas microservices architecture still have its challenges, the gradations of software architectures in software engineering industry can be analogous to the data engineering domain. One can perceive the pipeline architecture and its coupling nature similar to SOA and its practice of writing business logic in the SOA bus to connect the services. 

% Based on the premises discussed and overcome the limitations, we posit 4 underpinning principles for Cybermycelium;

% \begin{enumerate}
%     \item Distributed Domain-driven services with bounded context
%     \item Data as a service
%     \item Data infrastructure automation
%     \item Governance through a federation service
%     \item Event driven services

% \end{enumerate}

% \subsubsection{Distributed Domain-driven services with bounded context}

% Integral to Cybermycelium, is the distribution and decentralization of services into domains that have clear bounded context. Perhaps one the most challenging things one might face when it comes to architecting a distributed system is: based on what architectural quanta should we break down the system. This issue has been repeatedly discussed for example among adopters of microservices architecture . Cybermycelium, inspired by the concept of domain-drive design, tends to sit data close to the product domain that relates to it. This implies that data inheres in the product domain and as a facet of it \cite{laigner2021data}.

% This is mainly driven by the fact that most organizations today are decomposed based on their products. These products are the capability of the business that are segregated into various domains. Domain's bounded context is operated by various teams with different visions and concerns, incorporating data into a bounded context can result in a synergy that can improve the management of evolution and continuos change. This can be micro, such as application developers communicating with data engineers about collecting user data in a nested data structures or in flat ones, or macro, such as application developers thinking about redesigning their graphql schema in an intermediary layer that may affect the data engineers ingestion services.

% It is worth mentioning that, we are absorbing the concept of domain-driven design into this study to facilitate communication and increase the adoption, rigour and relevance of our RA. Communication is a key component of any software development endeavour \cite{sudhakar2012model}, and without it essential knowledge sharing can be compromised. Often data engineers and business stakholders have no direct interaction with one another. Instead, the domain knowledge is translated through intermediaries such as business analyst or project managers to series of tasks to be done \cite{khononov2021learning}. This implies at least two translations from two different ontologies. 

% In each translation, information is lost, which is the essential domain knowledge, and this implies risk to the overall data quality. In such data engineering process, the requirement often gets distorted, and data engineer has no awareness of the actual business domain and the problem being addressed. Often times, problems being solved through data engineering, are not simple mathematical problems or a riddle, but rather have broader scopes. An organization may decide to opimize workflows and process through continous data-driven decision making, and a data architecture that is overly centralized and not flexible can risk a project failure. 

% To this challenge, domain-driven design proposes a better approach to convey knowledge from domain experts to data engineers. In domain-driven design, instead of intermediary translations, business domains are projected into actual data engineering, emphasizing on creation of one shared terminology, that is the `ubiquitous language'. We do not aim to explore all facets of domain-driven design in this study, but it's worth mentioning that each business has it's own domain, and constituent core, generic and supporting sub-domains, and this varies from context to context.

% \subsubsection{Data as a service:}

% Data can be conceived as the fourth dimension of a product next to UI/UX, business and application as displayed in Figure \ref{data-facets}. Each domain provides its data as a service. This data is consisting of both operational and analytical data. This also implies that any friction and coupling between data is removed. For instance, the `invoice' domain will provide the transactional data about number of invoices and total of discounts with analytical data such as which practices have created what number of invoices in what period of time.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=10cm]{Media/product-facets.jpg}
%     \label{data-facets}
%     \caption{Product Facets}
% \end{figure}

% However this data-as-a-service model should be carefully implemented to account for explorability, discoverability, security, and quality. The data provided as a service should have the identical qualities to customer-facing products. This also implies, that a product owner should now treat data facet as an aspect of the product and employ objective measures that assure the desired quality. These measures can be net promoter scores from data consumers, data provenance, and decreased lead time. Product owners, in addition to the application and design aspect of the product, must now incorporate this new facet, and try to understand the needs of the data consumers, how they consume the data, and what are the common tools and technologies to consume the data. This knowledge can help shaping better interfaces for the product.

% Product domains may also need to ingest data from upstream domains, and this requires the definition of clear interfaces. Furthermore, each domain should also account for metadata. Metadata is derived from the nature of the product and its data lifecycle. Data can be ingested and served in various forms such as tables, graphs, JSON, Parquet, events, and many more; but in order for the data to be useful for analytical purposes, there is a need to associate data with its corresponding metadata that encompasses semantics, and history.

% \subsubsection{Data infrastructure automation}

% As the number of product domain increases, the effort required to build, deploy, execute, and monitor services increases. This includes the data pipelines required for that product domain to carry out its functions. The platform skills required for these kinds of work is usually found in Devops engineers and site reliability engineers. Application developers and data engineers are usually not adept at carrying out such workloads in an efficient manner. For this reason, there is a need for highly abstract reusable infrastructural components that can be easily utilized. This implies that teams should be equipped with required infrastructure as a service, that can be easily employed to account for BD needs.

% One way to provision such infrastructure as a service, is to utilize infrastructure as a code software tools like Terraform \cite{Terraform} and following the principles of GitOps. Besides, data infrastructure may be extended based on currently running infrastructure for application payloads. However, this might be challenging, as the BD ecosystem is growing rapidly, and while a software application might be running on a EC2 worker node in an EKS cluster on Amazon, the BD system maybe running on Databricks, or using a customer data platform (CDP) solution like Segment \cite{Segment}.This brings the challenge of composing data and application infrastructure together to provide a coherent, cost-efficient and interoperable infrastructure.

% Nevertheless, this should not be a daunting task, as one can simply extend the EKS confis and add a new pod to the network, which installs Databricks through a Helm Chart \cite{Helm}. In addition, the data infrastructure should be accompanied with proper tooling. Tools like GNU Make \cite{Make}, makes it quite easy for developers and data engineers to deploy infrastructure as they demand.
% A mature infrastructure as a service should provide the team with core infrastructures such as BD storage, stream processing services, batch processing services, event backbones or message queues, and data integration technologies.

% \subsubsection{Governance through a federation service}

% The other principle of Cybermycelium is the global governance or the global standardization of the services. This principle is perhaps a lesson learnt from the studied application of miroservices architecture in the industry \cite{alshuqayran2016systematic}. Distributed architectures are made up of independent collection of nodes, with distinct lifecycle that are deployed separately and are owned by various teams. As the number of these services grow, and the interconnections increase, the challenge of maintaining and scaling the system increases. This means services need to interoperate, ingest data from other services, perform graph or set operations in a timely manner and do stream processing.

% In order to scale and maintain these independently deployed yet interconnected services, Cybermycelium needs a governance model that embrace domain autonomy, decentralization, automation, Devops, and interoperability through federated government. This requires a shift in thinking, which obsoletes many prevalent assumptions of software and data engineering. The point of federation is not to suppress or kill the creativity and innovation of the teams, but rather, introduction of global contracts and standards that are in-line with company's resources and vision. Nevertheless, finding equilibrium between right amount of centralization and decentralization introduces challenge. For instance, semantic related metadata can be left to the product domain to decide, whereas policies and standards for metadata collection should be global. This is somewhat analogous to architectural principles in TOGAF's ADM \cite{josey2016togaf}.

% The definition of these standards is up to the architecture, or architectural governance group, and is usually achieved through service level objectives (SLOs) or well-defined contracts and standards.

% \subsubsection{Event driven services}

% Cybermycelium has been designed in a decentralized and distributed manner. Despite the advantages of decentralized systems in terms of maintenances and scalability, communication between the services remains a challenge. As the number of services grow, the communication channels increases, and this soon turns into a nexus of interconnected services that each try to meet its own end. Each service will need to learn about the other services, their interfaces, and how the messages will be processed. This increases the coupling between services and makes maintenance a challenging task. We argue that this should not be the aim of a distributed RA such as Cybermycelium.

% One approach to alleviate these issues is asynchronous communication between services through events. This is a different paradigm to a typical REST style of communication. A point-to-point communication occurs between services as series of `command', like getting or updating a certain resources, whereas event-driven communication happens as a series of events. This implies that instead of service A commanding service B for certain computation, service B reacts to a change of state through an event, without needing to know about service A.

% This provides a `dispatch and forget' kind of a model, in which a service is only responsible to dispatch an event to a topic of interest for the desired computation. In this way, the service does not need to wait for the response and see what happens after the event is dispatched, and is only responsible for dispatching events through a well-defined contract. Underlying this paradigm, services do not need to know about each other, but rather they need to know what `topic' they are interested in.

% This is analogous to a restaurant, where instead of a waiter needing to communicate directly to another waiter and to the chef and to the cook, they all react to certain events, such as customers coming in, or an order slip being left on a counter. The subtlety lies in the underlying paradigm and philosophy of `event' instead of `command'. This paradigm solves many issues of communication in distributed BD systems such as long running blocking tasks, throughput, maintenance, scale and ripple effect of service failure.

% It is worth mentioning, that eventual consistency (BASE) is preferred over ACID transactions for performance and scalability reasons. The detail of these two varying kind of transactions is outside the scope of this study.

% \subsection{The Artefact} \label{the-artifact}

% After having discussed many kernel and design theories, the necessary theoretical foundation is created for the design and development of the artefact. Cybermycelium is created with Archimate and displays the RA mostly in technology layer. Displaying these services in technology layer means that it's up to the designer to decide what flow and application should exist in each node. For the sake of completion, and as every software is designed to account for a business need, we have assumed a very simple BD business process. While this business layer could vary in different context, Cybermycelium should be able to have the elasticity required to account for various business models. To ease understanding of the RA, we sub-diagrammed the product domain in Figure \ref{fig:Cybermycelium Product Domain Design}.

% Cybermycelium is made up of 11 main components and 9 variable components as depicted in Figure \ref{fig:Cybermycelium}.




% \begin{sidewaysfigure}
%     % \begin{figure}[h!]
%         \includegraphics[width=23cm]{Media/Cybermycelium.jpg}
%         \caption{Cybermycelium BD Reference Architecture}
%         \label{fig:Cybermycelium}
%     % \end{figure}
% \end{sidewaysfigure}


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=12cm]{Media/Product Domain.jpg}
%     \caption{Cybermycelium Product Domain Design}
%     \label{fig:Cybermycelium Product Domain Design}
% \end{figure}


% The main elements are;

% \begin{enumerate}
%     \item \textbf{Ingress Service:} The ingress service is responsible for exposing the necessary port and endpoint for the data to flow to the system. Depending on the nature of the request, the ingress service will load balance to either batch processing controller or stream processing controller. It is essential for the ingress service to operate asynchronously to avoid any potential choke points. In addition, ingress handles the SSL termination, and potentially name-based virtual hosting. Ingress has several benefits. Firstly, it helps with security by preventing port proliferation and direct access to services. Secondly, it help with performance by distributing requests based on their nature, and SSL termination. Thirdly, if there's a need for object mutation through a proxy, ingress is the best architectural construct. Having an ingress also means that the point of entry is clear, which makes monitoring easier, and allows for other components of the architecture to remain in private networks. This component addresses the
%     requirements Vol-1, Vol-2, Var-1, Var-3, Var-4, Val-1,
%     Val-3, Val-4, SaP-1 and SaP-2.
%     \item \textbf{Batch Processing Controller:} Batch processing controller is responsible for dispatching batch events to the event backbone. This service should be a small service (could be a Lambda) with the main responsibility of receiving a request for batch processing and dispatching an event to the event broker. Because the nature of the request is of type batch and has been clearly distinguished by the ingress, batch processing controller can dispatch events in bulk and asynchronously. This is the main difference of this service to stream processing controller. Batch processing controller can execute other non compute-intensive tasks such as scrubbing properties from the given data or adding headers. Having a specific controller for batch processing improves monitoring and allows for customized batch event producing. This component addresses the requirements Vel-1, Val-1, and Val-2.
%     \item \textbf{Stream Processing Controller:} Stream processing controller is responsible for dispatching streaming events to the event backbone through the event broker. This service has been segregated from the batch service as it has to account for a different nature of events. Streams are synchronous in nature, and can require high-throughput. This service is a small service as well, but non-heavy computations such as enabling stream provenance, and one-pass algorithms can be utilized. Having a specific controller for stream processing means that custom attributes can be associated to stream events, and the events can potentially be treated differently based on the nature of the system. This also eases monitoring and discovery. This component addresses the requirements Vol-1, Vel-1, Vel-2, Vel-4, Vel-5, Val-2,
%     \item \textbf{Event Broker:} Event brokers are designed to achieve `inversion of control'. As the company evolves and requirements emerge, the number of nodes or services increase, new regions of operations may be added, and new events might need to be dispatched. As each service has to communicate with the rest through the event backbone, each service will be required to implement it's own event handling module. This can easily turn into a spaghetti of incompatible implementations by various teams, and can even cause bugs and unexpected behaviors. To overcome this challenge, an event broker is introduced to each service of the architecture. Each service connects to its local event broker and publishes and subscribes to events through that broker. One of the key success criteria of the event broker is a unified interface that sits at a right level of abstraction to account for all services of the architecture. Event brokers, being environmentally agnostic can be deployed to any on-premise, private or public infrastructure. This frees up engineers from having to think about the event interface they have to implement and how it should behave. Event brokers can also account for more dynamism by learning which events should be routed to which consumer applications.  Moreover, event brokers do also implement circuit breaking, which means if the service they have to broke to is not available and does not respond for a certain amount of time, the broker establishes unavailability of the service to the rest of the services, so no further requests come through. This is essential to preventing a ripple effect over the whole system if one system fails. This component indirectly addresses the
%     requirements Val-1, and Ver-1.
%     \item \textbf{Event Backbone:} This is the heart of the Cybermycelium, facilitating communication among all the nodes. Event backbone in itself should be distributed and ideally clustered to account for the ever-increasing scale of the system. Communication occurs as choreographed events from services analogous to a dance troupe. In a dance troupe, the members respond to the rhythm of the music by moving according to their specific roles. In here, each service (dancer) listens and reacts to the event backbone (music) and takes the required action. This means services are only responsible for dispatching events in a `dispatch and forget' model, and subscribe to the topics that are necessary to achieve their ends. Event backbone thus ensures a continues flow of data among services so that all systems are in the correct state at all times. Event backbone can be used to mix several stream of events, cache events, archive events, and other manipulation of events, so long as it's not too smart! or does not become a ESB of SOA architectures. Ideally, an architect should perceive the event backbone as series of coherent nodes that aim to handle various topics of interest. Over the time, event backbone can be monitored for access patterns and can be tuned to facilitate communication in an efficient manner. This component addresses the requirements Vel-1, Vel-2, Vel-3, Vel-4, Vel-5, Val-1, Val-2, Ver-1, Ver-2, and Ver-3.
%     \item \textbf{Egress Service:} The egress service is responsible for providing necessary APIs for the consumers of the system to request data in demand. This is a self-serve data model in which data scientists or business analyst can readily request data from various domains based on the data catalogue. Clients can first request for a data catalogue and then use the catalogue to request for the product domain that accounts for the desired data. This request can include several data products. Egress is responsible to route the request to the data catalogue, and to corresponding product `service mesh' in order to resolve values. The egress realizes the address to service meshes and other services through the data catalog and service discovery. The egress service should cache the resolved addresses and values in order to increase performance and response time. An architect can even choose the implement a complete query cache component inside the egress service, however that will incease complexity and can affect modifiability. This component is to avoid having people requesting directly to data engineers for various BD requirements, and means that people can just request for what data they need, analogous to a person who orders food at a restaurant; menu being the data catalog, and egress being the waiter. This component addresses the requirements Vel-2, Vel-4, Val-3, Val-4, SaP-1, and SaP-2.
%     \item \textbf{Product Domain Service Mesh:} As previously discussed, a product is a capability of the business, and each product has its own domain consisting of the bounded context and the ubiquitous language. From a system and architectural point of view, these domains are reffered to as a `service mesh'. Each service mesh is made up of a batch ingress, stream ingress, BD storage, BD processing framework, domain's data service, the required compute nodes to run these services, a side car per service and a control tower. These components provide the necessary means for the domain to achieve its ends. This architectural component removes the coupling between the teams and promotes team autonomy. This means people across various teams are enhanced with the desired computational nodes and tools necessary, and can operate with autonomy and scale without having to be negatively affected by other teams or having friction with platform teams or siloed data engineering teams. Depending on the context and the business, the architect may create several domains. This component indirectly addresses Vol-1, Vel-3, Vel-4, Vel-5, Var-1, Var-2, Var-3, Val-1, Val-2, Val-3, Val-4, Sap-1, SaP-2, Ver-1, Ver-2, and Ver-3.
    
    
%     \item  \textbf{Federated Governance Service:} Evidently, Cybermycelium is a distributed architecture that encompasses variety of independent services with independent lifecycle that are built and deployed by independent teams. Whereas teams have their autonomy established, in order to avoid haphazard, out-of-control and conflicting relations, there should be a global federated governance that aims to standardize these services. This will facilitate the interoperability between services, communication, aggregates, and even allows for a smoother exchange of members across teams. This also means the most experienced people at a company such as technical leads and lead architects will prevent potential pitfalls that more novice engineers may fall into. However the aim of this service is not centralize control in anyway, as that would be going a step backward into the data warehouse era. The aim of this service is to allow autonomous flow in the river of standards and policies that tend to protect company from external harm. For instance, failing to comply to GDPR while operating in europe can sets forth fines up to 10 million euros, and this may not be something that novice data engineers or application developers are fully aware of. The real challenge of the governance team is then to figure out the necessary abstraction of the standards to the governance layer and the level of autonomy given to the teams. The federated governance service is made up of various components such as global policies, metadata elements and formats, standards and security regulations. These components are briefly discussed below;
%     \begin{enumerate}
%         \item \textbf{Global Policies:} general policy that govern's the organizational practice. This could be influenced by internal and external factors. For instance, complying to GDPR could be a company's policy and should be governed through the federated governance service.
%         \item \textbf{Metadata Properties and Formats:} this is an overarching metadata standard defining the required elements that should be captured as metadata by any service within the organization; it can also include the shape of metadata and the properties of it. For instance, the governance team may decide that each geographic metadata should conform to ISO 19115-1 \cite{ISOMetadata}.
%         \item \textbf{Standards:} overall standards for APIs (for instance Open API), versioning (for instance SemVer), interpolation, documentation (for instance Swagger), data formats, languages supported, tools supported, technologies that are accepted and others.
%         \item \textbf{Security Regulations:} company wide regulations on what's considered secured, what softwares are allowed, how interfaces should be conducted and how the data should be secured. For instance, company may choose to alleviate risks associated with OWASP top 10 application security risks.
%     \end{enumerate}

%     While we promote above mentioned components as bare minimum, an architect may decide to omit or add a few more components to the federated governance service. This component can indirectly affect all requirements.

%     \item \textbf{Data Catalog:} As the products increases, more data become available to be served to consumers, interoperability increases, and maintenance becomes more challenging. If then, there is no automatic way for various teams to have access to the data they desire, a rather coupled and slow BD culture will evolve. To avoid these challenges and to increase discoverability, collaboration, and guided navigation, the service data catalog should be implemented. Data catalog is listed as a must-have by Gartner \cite{GartnerDataCatalog} and introduces better communication dynamics, easier data serve by services and intelligent collaboration between services. This component addresses the requirements Vel-4, Var-1, Var-3, and Var-4.
    
%     \item \textbf{Logging Aggregator and Log Store:} If all services employ the idea of localized logging, and simply generate and store logs in their own respective environments, debugging, issue finding and maintenance can become a challenging task. This is due to the distributed nature of Cybermycelium and the requirements to trace transactions among several services. In order to overcome this challenge, we've employed the log aggregator pattern popularized by Chris Richardson \cite{MicroServicesPatterns}. The log aggregator service is responsible for retrieving logging events through the event broker from individual services and write the collected data into the log store. The log aggregator configuration and semantics is up to the designer and architecture team. This allows for a distributed tracing, and graceful scaling of organizational logging strategies. This component indirectly addresses the requirements Vol-1, Vel-1, Val-1, and Ver-1.

%     \item \textbf{Event Archive:} As the quantity of services grow, the topics in event backbone increases, and the number of events surges. Along the lines of these events, there could be a failure, resulting in timeout and a loss of series of events. This brings system in a wrong state and can have detrimental ripple effect on all services. Cybermycelium tends to handle these failures by using an event archive. The event archive as the name states, is responsible for registering events, so they can be retrieved in the time of failure. If there was a blackout in certain geographical location and the event backbone went down, the backbone can recover itself and bring back the right state of the system by reading the events from the event archive. Th event broker is responsible for circuit breaking, so services do not request any more events to the backbone while its down. The time to expiry, and what events should be archived is decided based on context in which Cybermycelium is implemented. This component indirectly addresses
%     the requirements Vol-1, Vel-1, Val-1, and Ver-1.

%     \item \textbf{Data Lake:} Whereas Cybermycelium is a great advocate of decentralized and distributed systems, we do not find it necessary for each product domain to have its own kind of a data lake or data storage. This is to prevent duplication, contrasting data storage approaches, decreased operability among services and lack of unified raw data storage mechanisms. Data lake has been designed to store large volume of data in raw format before it can get accessed for analytics and other purposes. This means data can be first stored in the data lake with corresponding domain ownership before it needs to be accessed and consumed by various services. Structured, semi-structured, unstructured and psudo-structured data can be stored in data lake before it gets retrieved for batch and stream processing. Nevertheless, this does not imply that all data should directly go to the data lake; the flow of data is determined based on the particularities of the context in which the system is embodied. One approach that we find suitable is for each team to own a unit of storage in the data lake, which is handled by the access control. This component addresses the requirements Vol-2, Vel-1, Var-1, Var-3, Var-4, Val-3.
    

%     \item \textbf{Service Discovery:} In a distributed setup like Cybermycelium, how do services discover the location of other services? This is achieved through service discovery. As the practice of hard-coding service addresses in configuration files is not a maintainable or scalable approach, one has to think about an automated scalable solution in which services can become discoverable by other services. The service discovery node is responsible for this job. This is achieved through services registering themselves to the service discovery node when they boot up. Service discovery then ensures that it keeps an accurate list of services in the system, and provides the API necessary for others to learn about the services. For instance, it's idiomatic for an engineer to specify a command to be executed when a Docker container starts (\emph{Node server.js}); thus one can imagine extending the boot up instructions to achieve the registration to the service discovery node. This somewhat resembles to DHCPs and house wifi networks.  This component indirectly addresses the requirements Vel-2, Vel-4, Var-2, Var-4, Val-3, Val-4, SaP-2.

%     \item \textbf{Monitoring:} Monitoring systems are integral to robustness of highly dynamic ecosystem of distributed systems and directly affect metrics such as mean time to resolution (MTTR). Services emit large amounts of multi dimensional telemetry data that covers a vast spectrum of platform and operating system metrics. Having these telemetry data captured, handled and visualized, helps systems engineers, software reliability engineers, and architects proactively address upcoming issues. Based on these premises, the main responsibility of this service is to capture and provide telemtry data from other services to increase the overall awareness of the Cybermycelium ecosystem. This service is tightly aggregated with the service discovery. Monitoring services help storing
%     these data to fuel proactive actions. This component
%     indirectly addresses all requirements.

% \end{enumerate}

% The variable elements in Cybermycelium can be adjusted, modified and even omitted based on the architect's decision and the particularities of the context. The aim of this RA is not limit the creativity of data architect, but to facilitate their decision making process, through introduction of well-known patterns and best practices from different school of thoughts. While we still recommend keeping the variable components, an architect may decide to embark on a more complicated metadata approach rather than just a data catalog. We do not elaborate on all alternative options for each variable module as industry constantly changes, and architects constantly aim to design systems that address the emerging problem domains. 


% \section{Evaluation:} \label{evaluation-section}

% Of particular importance to development of a RA, is the evaluation of it. As discussed earlier, we aim to evaluate the correctness and utility of the RA by how it can be turned into an effective context-specific concrete architecture, following the guidelines of ATAM. The main goal of ATAM is to appraise the architectural decisions and their consequences in light of quality attributes. This method ensures that the architecture is under the right trajectory and in-line with the context. Architecture is a an amalagamation of risks, tradeoffs, and sensitivity points. Using ATAM increased our confidence by uncovering key architectural tradeoffs, risks, and sensitivity points.

% For ATAM to be successful there should not be a precise mathematical analysis of system's quality attributes, but rather trends should be identified where architectural patterns are correlated with a quality attribute of interest. For brevity purposes, we do not expand on what ATAM is, and the details of each steps in it, and we only explain how the evaluation has been conducted through ATAM. It is important to note that, this wasn't a setup in which an outside evaluation team would come to a company to evaluate an architecture in practice, but it was our prototype that we brought into a company to test its utility and relevance. 

% While we could have achieved this with technical action research or leight weight architecture evaluation, we found ATAM to be in-line with our conceptual constructs, which are architectural constructs. ATAM provided us with a framework to discuss architectural concepts in a rigorous way \cite{wieringa2014design}. We created the prototype, and played the role of the evaluator, thus there was a risk of bias. To avoid bias, we invited a third-party researcher who is familiar with ATAM to observer the overall process and partake in architectural probing questions. 

% For instantiation of the RA, We utilized ISO/IEC 25000 SQuaRE standard (Software Product Quality Requirements and Evaluation) \cite{ISO25000} for technology selection.  We did not fully adopt this standard, but were rather inspired by it to make a better decision. The quality model in the standard is based on characteristics, sub-characteristics and standards. This standard also references many other standards for maintenance and quality keeping of computer systems. The detailed explanation of the standards and its constituent elements are outside the scope of this study.

% By applying standard and the requirements of Cybermycelium to the pool of available tools in the industry, we succesfully created the prototype. We chose the tools are the mostly adopted and do support the architectural requirements of Cybermycelium. We did not want to develop tools from scratch, as that would delay our evaluation artefact and this would affect the stakehodlers negatively. In addition, many mature tools exist that statisfy our architectural requirements, so therefore 'reinventing the wheel' was unnecessary.

% We chose Node JS for all APIs and custom scripting, Nginx as our ingress, AWS Lambdas for stream and batch processing controllers, Kafka for event backbone, Kafka event brokers as the event broker, AWS application load balancer as the egress load balancer, Istio as the control tower, Envoy as the side car, Kubernetes as the container orchestrator, AWS S3 as the BD store and event archive, and Data Bricks for stream and batch processing. The prototype created out of Cybermycelium is depicted in Figure \ref{fig:ConcreteCyberMycelium}

% \begin{sidewaysfigure}
%     % \centering
%     \includegraphics[width=23cm]{Media/concrete-mycelium.jpg}
%     \caption{Cybermycelium Instantiation}
%     \label{fig:ConcreteCyberMycelium}
% \end{sidewaysfigure}

% We aimed to incorporate most components of our RA into this instance, however logging, monitoring, service discovery, federated governance service, and data catalog has been omitted. Some details of this evaluation is omitted to protect the security, and intellectual property of the practice, and some details are modified for academic purposes. These modifications have not affected the integrity of the evaluation. This evaluation is an iterative process, collecting feedback from different group of stakeholders in each phase. 

% \subsection{Phase 1:}

% This evaulation is undertaken in a subsidiary of a large-scale international company that has over 6000 employees all around the globe. The subsidiary company offers a pratice management software for veterinary practitioners via Software as a Service (Saas) and has over 15,000 customers from the USA, UK, Australia, New Zealand, Canada, Singapore and Ireland, among which are some of the biggest equine hospitals, universities and veterinary pracices. The company is currently at the stage of shifting from centralized synchronous architecture into decentralised event driven microservices architecture, and is ambitious to adopt artificial inteligence and BD.

% The initial step was the identification ot relevant stakeholders. For this purpose we have approached the key stakeholders in the company's technical governance team. Our aim was to incorporate at least two lead architects of the company in this process. We emphasized on architects that have been with business for a long period of time. This was to ensure that we do not miss any important element in the process of evaluation. As a result, we invited two lead development architects, head of product, and a business analyst for phase 1.


% \subsubsection{Step 1 and 2: Introduction}

% During the initial meeting, in step 1, ATAM was presented with clear description of its purposes. In step 2, stakeholders discussed the background of the business, some of the challenges faced, the current state of affairs, the primary business goals, and architecturally significant requirements. This step illuminated on integral elements such as: 1) most important functions of the system, 2) any political, regional, or managerial constraints, 3) the business context and how it relates to our prototype, 4) architectural drivers.


% \subsubsection{Step 3: Present the Architecture}

% In step 3, the prototype has been presented, our assumptions have been stated, and variability points portrayed. 

% \subsubsection{Step 4: Identifying Architectural Approaches}

% To establish the architectural styles, we first analyzed the prototype with regards to architectural patterns and princinples depicted in Section~\ref*{theory-section}. We then digged a bit deeper and justified our architectural decisions. We discussed the event-driven nature of the prototype, and discussed the usefulness of the domains.

% \subsubsection{Step 5: Utility Tree Elicitation:}
% In order to generate the utility tree, first we had to learn what are the most important quality attributes. While we learnt about these quality attributes in step 2 shortly, in this step we probed deeper. We first presented our assumptions, and double-checcked it with the stakeholders. Whereas some stakeholders raised concerns over privacy, the members unanimously agreed that performance, availability, and maintainability are the most important quality attributes. This was in-line with our assumptions. In this process, we rated the technical difficulty, and the key stakeholders rated the business importance.

% Based on these premises, the utility tree has been generated (Figure \ref{fig:utility-tree} ).

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=10cm]{Media/Utility-tree.jpg}
%     \caption{Utility Tree}
%     \label{fig:utility-tree}
% \end{figure}

% \subsubsection{Step 6: Analyze Architectural Approaches:}

% After having scenarios prioritised and architectural approaches identified, it's time to analyze if the architectural approaches are the fitting for the given scenarios. In this step, we asked the lead architects to probe the architecture and we explained how the prototype is addressing each scenario. 

% We justified our architectural constructs by evaluating key quality attributes we've collected previously for the purposes of this evaluation. We explained the following for each quality attribute:

% \begin{itemize}
%     \item For performance, Nginx, Kafka, Istio, DataBricks and the AWS Application Load Balancer have been described.
%     \item For availability, Kafka, Event archive, Nginx, controllers, Data Lake and Istio have been discussed.
%     \item For modifiability, the concept of domain-driven design, the service mesh, zere coupling, the plug and play nature of the archetype, the ability to add desireable services through event brokers, and the distributed nature of the architecture has been discussed.
% \end{itemize}

% The result of this step, was identification of sensitivity points, trade-offs, risks and non-risks. This step took longer than what we anticipated, as variety of questions arose and many aspects of the architecture was challenged. The detail is discussed in Section~\ref{evaluation-results}. 
 

% \subsection{Phase 2:}

% This phase was a more serious phase of our evaluation, as we invited more stakeholders, collected more scenarios and even created simulations. For this phase, in addition to lead architectes, we invited a product owner responsible for the product in which the artefact is tested, a quality assurance engineer and several developers. We repeated step 1, and provided with recap of step 2 to 6, and shared the current list of risks, non-risks, sensitivity points, and tradeoffs. 

% This phase is an iteration of phase one, so we collected scenarios, analyzed architectural approaches, and finally presented the result of the evaluation. 

% \subsubsection{Step 7: Brainstorm and Prioritize Scenarios}

% While utility tree was created in step 5, the purpose of this brainstorming was to get a pulse of the larger stakeholder community and capture more scenarios. Scenarios are the quantum of ATAM, and help captring stimuli to which the architecture has to react. These stimuli help to highlight system's ability to too meet desired functional and non-functioanl requirements \cite{KazmanATAM}. 

% Based on this premise, in this step, we asked stakeholders to come up with three different kind of scenarios namely growth scenarios (anticipated changes), use-case scenarios (typical usage of the system) and exploratory scenarios (extreme cases). The result of this created 20 scenarios, which we then asked stakeholders to vote on. The result of the voting yielded 5 scenarios which are described as two user journeys;

% \begin{itemize}
%     \item A cat owner brings a cat to the veterinary hospital. The cat has symptoms of a lyme disease, and should be diagnosed in a timely manner to avoid master problems.
%     \item There has been numerous cases of cancer in pets in certain environments. This environment should be analyzed to see if environmental factors play a cancer inducing role.
% \end{itemize}

% \subsubsection{Step 8: Analyze Architectural Approaches}

% Before starting this step, we took a few days break to simulate the scenarios against our prototype. While ATAM does not prescribe this, we augmented our evaluation with this simulation to ensure that we do not overlook any necessary architectural detail. This improved our confidence in our RA and the architectural probing questions to come. 

% We emulated the scenarios against the prototype by creating relevant topics in the Kafka, having the data flow, having the ingress in the service mesh digest it and flow it into the storage and processing and so on so forth. We've been using real world data, so there was no need for data fabrication and synthesis. We configured Nginx to pass the request to the responsible Lambdas, and Lambdas then produced the necessary events and sent it to Kafka. 

% We presented our simulation alongside some metrics we captured and displayed in our cloud served Garafana instance. From here on, this step followed the exact same procudure as step 6, with a difference that this time there's been more extensive probing and analysis of the architecture and the simulated scenarios. The simulation and results of it helped clarifying on some of our architectural constructs and led to emergence of several questions: 

% \begin{itemize}
%     \item How does the system reocover if the event backbone goes out of order?
%     \item What if the service mesh ingress is not available?
%     \item Should privacy be it's own service? or should it sit in federation ?
%     \item Should have a dedicated service mesh for metadata management?
%     \item How easy it is to extend and modify current services?
%     \item Should there be a certain order to events ?
%     \item Is there a benefit in creating event mesh between event brokers?
%     \item Where is the best place to scrub sensitive data from the incoming streams?
% \end{itemize}

% \subsubsection{Step 9: Present Results} \label{evaluation-results}

% In this last step, the collected theories form the process of evaluation is discussed in terms of quality attributes, risks, sensitivity points, tradeoffs and other unplanned discussions that arose during the meetings. 

% Based on the result of our evaluation, stakeholders feedback, utility tree, and the architectural qualities of Cybermycelium, we deduce that system quality Q\textsubscript{S}, is a function f of the quality attributes availability Q\textsubscript{A}, performance Q\textsubscript{P}, and modifiability Q\textsubscript{M}.

% \begin{equation}
%     Q\textsubscript{S} = f(Q\textsubscript{M}, Q\textsubscript{A}, Q\textsubscript{P})
% \end{equation}

% \paragraph{Performance} In order to analyze our approach in-line with the utility tree, after having created the simulated scenarios, we used a cloud stress testing testing agent (StressStimulus). After having run this stess test a couple of times, it has become evident to us that cold start latency of AWS Lambda services can affect the performance requirements stated in the utility tree. A Lambda can take anywhere from 100ms to over a second on cold start time. This latency varies and hard to nail down, but even considering the latency, we have captured an average of 1000ms response time from our system which is inline with the utility tree. While replacing Lambdas with EC2s or Fargates could solve this issue, it would increase the cost, affect the maintainbility of the architecture (a server has to be provisioned and maintained), and would require rework of several services.

% In addition, other Lambda like solutions exist that have actually sovled the cold start problem; one good example is cloud workers offered by CloudFlare. However the company chosen for the purposes of this evaluation is not yet open to a multi-cloud approach, and thus AWS is the only option. Moreover, one could implement predictable start-ups with provisioned concurrency, but that requires more effort and is outside the scope of this study. As our architecture is distributed, we have also measured the latency in between services as tail latency is a known issue in distributed systems. Due to the fact that our service mesh was hosted in a private network on a virtual cloud, we could not find any major issue with cloud latency, and our average response time was under 1000ms. Implementing a streaming processing in Databricks, we opted not to use micro-batch to have an accurate evaluation, and we decided not to configure the fair scheduling pool, so as to test the worst case scenario.

% After creating and analyzing various performance models of the system, it has come clear to us that latency, and side-effects like input/output, and mutation/transformations were the most important performance sensitivity points. Our performance model were built underlying the following cases;

% \begin{itemize}
%     \item Priodic, regular data dispatch to the product domain
%     \item Sending large volume of data (over 200mb) to the system, reaching the throughput threshold
%     \item Sending many request simutaneously through the cloud stress testing tool
% \end{itemize}

% The event-driven nature of the system really helped with handling throughput and concurrency. Whereas there has been bottlenecks in the areas of storage and network latency, the system managed to reach desired performance on average. Given this insight and after some rigorous testing, we characterize the system's performance sensitivity as follows;

% \begin{equation}
%     Q\textsubscript{P} = h(s, l, cbp)
% \end{equation}

% That is the system is sensitive to side effects (s), latency (l), and concurrency back pressure (cbp).

% \paragraph{Availablity:}

% As guided by the utlity tree, the key stimulus to model for the prototype is the failure of the ingress (load balancer), the data processing cluster and most importantly the event backbone. Due to the distributed nature of Cybermycelium, and the derived prototype, failure in one service, if not handled properly, can have a ripple effect on the system. This is one area, where we found the idea of 'event brokers' really helpful. By implementing circuit breakers in event brokers, we prevented the other nodes of the system to be affected by failure of one. We also archived the events that the node was about to receive before it failed.

% Whereas the event archive has played an ancillary role in providing archive to various circuit breakers, it's main functionality was to provide event history to the event backbone in the case of failure. This is again achieved by circuit breaking at the broker level and event retrieval from the event archive. On the other hand, in relation to container orchestration and health check, Kubernetes provided with a declarative API to handle the state of the system. With setting replica sets, and necessary deployments, the master node kept ensuring that certain number of pods are always available. This implies that it's critical for master node to be available at all times.

% Based on these findings, we characterized system's availability as following (g is fraction of time that system is working);

% \begin{equation}
%     Q\textsubscript{A} = g(\lambda\textsubscript{E} , \mu\textsubscript{C}, \mu\textsubscript{S})
% \end{equation}

% That is, system availablity is primarily affected by the failure rate of the event backbone ($\lambda \textsubscript{E}$), the time it takes for circuit breaker to trip and become available again ($\mu\textsubscript{C}$), and the time it takes for the service to recover from failure ($\mu\textsubscript{S}$).

% One major factor that really helped alleviating many issues of the distributed systems, was the cloud-native aspect of Cybermycelium. Whereas this aspect of the architect has not beed discussed previously, the prototype was easily deployed in AWS with well-known Amazon web sevices. As we did not handle on-premise data centers, many of the hardware was handled by the cloud company.

% \paragraph{Modifiability:}

% To analyze modifiability, we followed the guidelines of SAAM \cite{kazman1994saam}. The distributed and service driven nature of the prototype allowed us to easily achieve the utility tree and even further. All of our cloud based infrastructure has been written as Terraform code in HCL, which meant adding a new node in the system, was as easy as copying the worker groups block in the EKS configuration, and setting the hardware properties of it. We could then easily deploy different services and deployments and have them run our public docker images. Brokers were also streamlined, and we could spin up a new broker within minutes. One area that we found a bit challenging to modify was perhaps the Databricks cluster, and the EKS ALB ingress (Nginx).

% Certification management was also easily handled through Istio, local CertManager and Let's encrypt. One area that could be taking a bit longer was the inclusion of private docker image secrets as a Kubernetes secret, and having it refreshed every 12 hours. To the best of our knowledge, cron jobs were the only way to achieve this, but the implementation was not straight forward.

% On the other hand, bringin up a scalable Kafka cluster was not that difficult, but there were so many configurations that one can choose to turn on or amend. This can potentially affect modifiability in the long run, when the company might have varying and sometimes conflicting requirements.

% Modifiability is also affected by the skillset of the engineers and how familiar they are with Kubernetes, Databricks and Istio. Taking all these into consideration, we characterize system's modifiability as follows (s is the skill set required);

% \begin{equation}
%     Q\textsubscript{M} = s(K, D, K)
% \end{equation}

% That is, the system modifiability is affected the Kubernetes maintenances (K), Databricks maintenances, versioning and configuration (D), and Kafka versioning, maintenance and configuration.


% \paragraph{Tradeoff Points:}

% As a result of these analyses, we identifed two tradeoff points;

% \begin{enumerate}
%     \item Event backbone and event brokers
%     \item Service mesh
% \end{enumerate}

% One area that arose many worries is the event backbone. Event backbone being the communication facilitator has raised a lot of questions and many worried that this might turn into a bloated architectural component like enterprise service bus (ESB) in the service oriented architectures (SOAs). We addressed many of these questions and issues both in a dicussion and the prototype. Implementing event archive meant that if the event backbone went down, we could restore the previous state of affairs and bring services to the correct state. The implementation of circuit breakers through the event brokers further solidifed the avaiability of the architecture and could deem to affect reliability too. Along the lines, event brokers helped us address some of the modifiability challenges. Having these event brokers setup, meant that different environments do not implement their own event processing mechanism, and the interface is unified across. This clear interface contributed positively to the overall modifiability of the system and allowed engineers to simply copy the broker for their services. In addition, brokers also improved interoperability, and hard to trace bugs duge to event processor missmatch.

% Given all, Cybermycelium does not tend to dictate what has to be done, or kill the creativity of the archites, but rather aims to shed ligts on a novel perspective to designing BD systems. Therefore, the event backbone and event brokers introduce tradeoff between performance, availability and reliability. Whereas eliminating the event backbone may increase avaiability longitudinaly, and increase modifiability cross-sectionally, it may affect the performance quality attribute in a negative way. This is due to the fact that the event backbone is distributed in nature, can scale well to account for demans, can cache and remember communication paths, merge event streams, provide with windowing techniques, and be configured to facilitate certain access patterns that are common to the system.

% Anothe area where stakeholders were challenged was the idea of service mesh. Whereas this makes a lot of sense to developers who had to figure out the twisted platform work, the benefit perhaps was not that evident to everyone from the beginning. This is another area of tradeoff. While having the service mesh affects the modifiability of the system in a negative way from platform point of view, it does increase it from data engineering and software engineer point of view. The service mesh may also affect performance slightly, but the effect is negligible. Service mesh also affects avaiability positively by streamlining the platform interfaces, providing an orchestrator ( control tower ), and doing health checks through proxies.

% \paragraph{Limitations}

% Cybermycelium is a new perspective to BD system development and tends to absorb many of the well-established patterns and ideas from various domains. Being distributed in nature, there are still many areas in which Cybermycelium can improve. For instance, we still don't have a great answer to tail latency issues which can affect system negatively. Besides that, we recived feedbacks that many developers find Cybermycelium a complex architecture that require a lot of skill to implement. It requires the understanding of event-driven systems, event streaming, service meshing, data mesh, cloud computing and even data mesh. We do not think that a modern distributed BD architecture should be simple, but we thrive to simplify the ways in which one can absorb Cybermycelium.

% Taking all into consideration, we posit that distributed BD systems are still at infancy stage, and there's much work required to facilitate this area of research. These research could be in the areas of BD distributed patterns, event driven BD systems, data mesh, BD reference architecturse, and methods for creating BD distributed architectures.

% Moreover, the security, privacy and metadata aspect of BD neeeds substantial work at macro and micro level. We need more mature technologies and better architectures that compose these technologies in a solution. This is one major area we have on our roadmap.

% \section{Discussion} \label{discussion-section}

% Our findings from this study yielded the fact that progress is uneven in the area of BD RAs. While there are many researches in the area of data warehousing, artificial intelligence, data science, and IOT, data engineering seems to be needing more research. While, there are many well established approaches for crunching large volume of data, or handling dimensionality of complex data sets, the overall organization of BD technologies, which is the architecture, needs more attention from academia and industry. 

% Majority of the BD RAs that we have analyzed were running underlying some sort of a monolithic data pipeline with a central storage. This is a challenging architecture to scale and maintain. How does one takes preventive measures to stop a data lake from turning into a data swamp ? How a team of hyper-specialized siloed data engineers that are running the data pipelines, will be aware of the actual business problem and therefore keep a certain level of quality of that data? how data interoperability is achieved? how data ownership is institutionalized ?

% If a software engineers decides to, for instance, manipulate a certain field in a certain entity's schema for the development of a new feature, how will this affect the data engineering process and how is this communicated? as data becomes more and more available to the company, the ability to consume it all in one place diminishes. 

% On the other hand, the current state of BD RAs do not seem to be very far away from traditional data warehousing approaches. In fact, some of them have adopted the idea of data marts and propose them as BD solution, but using newer technologies. Moreover, some architectures have attempted to utilize data lake to serve data analysts and business intelligence. 

% We posit that neither the attempt to onboard BD analytics workloads to data warehouses, nor the attempt to serve business intelligence with data lakes is gonna result in a scalable and maintainable system. We therefore propose the need for future research directions in the area of decentralized and distributed BD RAs. 

% We also felt that the quality of many of BD RAs published does not seem to be enough to meet the industrial expectations. This is due to the challenges of developing BD RAs and the cost and resources required to evaluate these artifacts. It is also worth mentioning that a rigorous methodology for evaluating RAs are quite rare, and while there are studies that have attempted to address these issues \cite{angelov2008towards}, there is a need for more research in this area.

% Given all, we posit that RAs can be considered an effective initial point to design and development of BD systems. These artifacts helps facilitating communication, capture requirements from various stakeholders, and catch design issues while they are still cheap. Based on this, therefore, more and more attention needs to be given to this area and its foundational methodological needs. This study does not aim to do a deep comparison of Cybermycelium and other RAs, the major architectural constructs, the challenges associated to current BD RAs, and the reasoning behind our artefact should elucidate the varying nature of our artefact.

% \section{Conclusion} \label{conclusion-section}



\section*{Appendix}
% Text for this section\ldots

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{}


% \section*{Funding}%% if any
% Text for this section\ldots

% \section*{Abbreviations}%% if any
% Text for this section\ldots

% \section*{Availability of data and materials}%% if any
% Text for this section\ldots

% \section*{Ethics approval and consent to participate}%% if any
% Text for this section\ldots

% \section*{Competing interests}
% The authors declare that they have no competing interests.

% \section*{Consent for publication}%% if any
% Text for this section\ldots

% \section*{Authors' contributions}
% Text for this section \ldots

% \section*{Authors' information}%% if any
% Text for this section\ldots

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{vancouver} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliography{mybibfile}      % Bibliography file (usually '*.


\end{backmatter}
\end{document}





